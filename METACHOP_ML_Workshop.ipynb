{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was provided by Dr. Ryan Urbanowicz to be used during the Math, Engineering and Technology at CHOP (METACHOP) Workshop on June 19th, 2019.\n",
    "\n",
    "*** \n",
    "# METACHOP Workshop\n",
    "* Contributors: Jorge Guerra, Helen Loeb and Remo Williams\n",
    "*  Affiliation: Children's Hospital of Philadelphia\n",
    "* Date: 05/20/2019\n",
    "* github: https://github.com/guerrajorge/metachop\n",
    "* google collab notebook: https://colab.research.google.com/github/guerrajorge/METACHOP/blob/master/METACHOP_ML_Workshop.ipynb\n",
    "***\n",
    "# Machine Learning (ML) 102 Workshop\n",
    "* Author: Ryan Urbanowicz, PhD (Debugging assistance by Dr. Trang Le)\n",
    "*  Affiliation: University of Pennsylvania - Department of Biostatistics, Epidemiology, and Informatics & Institute for Biomedical Informatics (IBI) in collaboration with the Leonard Davis Institute (LDI)\n",
    "* Date: 5/1/19\n",
    "* github: https://github.com/UrbsLab/ML_Pipeline_Notebooks\n",
    "\n",
    "A recording of Machine learning 101 Workshop (Dec 2018) – An Introduction to Machine Learning is available at:  \n",
    "https://bluejeans.com/playback/s/PF3d7xdm3DSBbZgHw6JpHnhoVSPVg2ACytbA6eMKFHRWEXSV2UaFNHMXJn7GV9kN\n",
    "\n",
    "A recording of Machine Learning 102 Workshop (May 2019) – Machine Learning: An Analysis Pipeline is available at:\n",
    "https://bluejeans.com/playback/s/sYL8Nfeq9M1H42nLcGPxuxc59aj1DZI6o3Qf8EYnApXP1W2vnphICfuuxlsokPIF\n",
    "\n",
    "To easily view this Jupyter Notebook in Google collab, use the following link: \n",
    "https://colab.research.google.com/github/UrbsLab/ML_Pipeline_Notebooks/blob/master/ML_102_Workshop.ipynb\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/python_jupyter.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<img src=\"images/python.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<img src=\"images/googlecolab.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Introduction\n",
    "This notebook presents an example of a machine learning analysis pipeline from start to finish. It was written to be paired with the ML 102 Workshop presented in collaboration with IBI and LDI, and it was modified for the CHOP METACHOP workshop. Please note that this notebook is meant to present an accessible example, but does not necessarily include the optimal strategies to analyze the target dataset examined herein. Identifying the optimal analysis pipeling steps/components is one of the fundamental challenges of data science.  This is almost never known ahead of time when seeking to tackle a new dataset/anlaysis. The pipeline presented below could be reproduced using different software or coding languages.  We have opted to utilize Python and the Jupyter notebook framework here due to is accessibility, flexibility, and prevalence in the ML community. \n",
    "\n",
    "<img src=\"images/ds_pipeline.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The following warning will be displayed when trying to run the notebook:\n",
    "\n",
    "<img src=\"images/authorization.png\" />\n",
    "\n",
    "Unselect __\"Reset all runtimes before running\"__ and Click on __\"RUN ANYWAY\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to install the following packages (not naitive to google colab)\n",
    "!pip install skrebate\n",
    "\n",
    "#Basic Packages\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from scipy.stats import randint\n",
    "import re\n",
    "\n",
    "#Scikit-Learn Packages:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif # Mutual information for a discrete target.\n",
    "from sklearn import tree #import decision tree package\n",
    "from sklearn import metrics #import evaluation metric package\n",
    "from sklearn.ensemble import RandomForestClassifier #import decision tree package\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# In-depth data exploration package\n",
    "import pandas_profiling\n",
    "\n",
    "\n",
    "#ReliefF feature selection package\n",
    "from skrebate import ReliefF\n",
    "\n",
    "#Visualization Packages:\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline   \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import _tree\n",
    "from matplotlib import pyplot\n",
    "import graphviz \n",
    "\n",
    "\n",
    "# # Jupyter Notebook Hack: This code ensures that the results of multiple commands within a given cell are all displayed, rather than just the last. \n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#Set a random seed for the notebook so that individual runs of the notebook yield the same results\n",
    "randSeed = 99 #changing this value will potentially change the models and results due to stochastic elements of the pipeline. \n",
    "np.random.seed(randSeed)\n",
    "\n",
    "#After running this cell, any error message here will inform you what package still needs to be installed on your system using pip install or conda install."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# RAW DATA\n",
    "Here we describe our target dataset, load it, and examine some basic properties of the data.  This examination of the data can be considered part of the exploratory analysis.  We have included it in this first section to provide a more logical flow to this analysis pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Description of Raw Target Dataset\n",
    "\n",
    "\n",
    "For the purpose of this notebook we have selected an accessible open source dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). Specifically we will apply our ML pipeline to classification dataset gathered to try and predict one year survival in patients with hepatocellular carcinoma (HCC). [Target Dataset Source](http://archive.ics.uci.edu/ml/datasets/HCC+Survival).\n",
    "\n",
    "<img src=\"images/hcc_dataset_description.png\" />\n",
    "\n",
    "* OpenSource: https://archive.ics.uci.edu/ml/datasets/HCC+Survival\n",
    "\n",
    "Prior to loading the data here we opened the dataset in excel as well as in a text editor noting the following about the dataset: \n",
    "1. The data has comma separated values (i.e. csv format).\n",
    "2. There is no header (i.e. column labels) in the data.\n",
    "3. A secondary data dictionary file is available that describes the features and includes the header values\n",
    "4. Missing values are denoted with '?'\n",
    "5. From the dictionary file we know that the class/outcome column is named 'Class Attribute'.\n",
    "6. Oddly the minority class is coded as 0 (patient died), and the majority class is coded as 1 (patient alive).  This is because the 'target' event in this data is 'patient survived 1 year'. \n",
    "\n",
    "We have created a csv file of the header names in excel taken directly from this data dictionary.  GitHub links to the data files needed for this notebook are included below:\n",
    "\n",
    "* [Header File](https://raw.githubusercontent.com/guerrajorge/METACHOP/master/data/HCC_headers.txt)\n",
    "* [Data File](https://raw.githubusercontent.com/guerrajorge/METACHOP/master/data/hcc-data.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load header names file using the pandas package: import pandas as pd\n",
    "# use the 'raw location' url to access the files from github \n",
    "header_file = 'https://raw.githubusercontent.com/guerrajorge/METACHOP/master/data/HCC_headers.txt'\n",
    "headers = pd.read_csv(header_file, sep='\\t',header=None) \n",
    "print('Number of (rows, columns)')\n",
    "print(headers.shape)\n",
    "print('First 5 columns:')\n",
    "print(headers.values[0,:5].tolist())\n",
    "header_list = headers.values[0].tolist() #creates a list variable from the first element dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and provide header names from above.\n",
    "target_data_file = 'https://raw.githubusercontent.com/guerrajorge/METACHOP/master/data/hcc-data.txt'\n",
    "hcc_df = pd.read_csv(target_data_file, na_values='?', header=None, names=header_list) # Data loaded so that the \"?\" cells are 'NA'\n",
    "print('Number of (rows, columns)')\n",
    "print(hcc_df.shape)\n",
    "print('First 5 instances')\n",
    "hcc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances = hcc_df.shape[0] # number of examples\n",
    "num_features = hcc_df.shape[1] # number of features\n",
    "\n",
    "print('Dataset contains {0} instances.'.format(num_instances))\n",
    "print('Dataset contains {0} features plus 1 class/outcome.'.format(num_features-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<img src=\"images/preprocessing.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Exploratory Analysis\n",
    "Run some basic Pandas commands to examine/confirm dataset properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the first 5 rows of the loaded data\n",
    "hcc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember class/outcome column is named '__Class Attribute__' coded as 0 (patient died), and  1 (patient alive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage\n",
    "hcc_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Assess Missingness in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hcc_unique_missing.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate missingness and data availability\n",
    "print(\"Missing Value Counts\")\n",
    "missing_count = hcc_df.isnull().sum()\n",
    "missing_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We confirm here that there is no missing data in the class/outcome variable. If there had been we would have to remove any rows with missing outcome later in the cleaning section. This is because we are performing supervised learning, i.e.  (label/outcome) required for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine the number of unique values for each variable/feature. Note that missing values are not being include as unique values. \n",
    "unique_count = hcc_df.nunique()\n",
    "unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Plot a histogram of these unique variable counts. \n",
    "ax = unique_count.hist(bins=num_instances,figsize=(16,4))\n",
    "ax.set_xlabel(\"Unique Value Counts\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Histogram of Unique Value Counts In Feature Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We observe that nearly half the features are binary, and there are some features that appear to be discrete interger values, and a number of real-valued features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a histogram of the missingness observed over all features in the dataset\n",
    "ax = missing_count.hist(bins=num_instances,figsize=(16,4))\n",
    "ax.set_xlabel(\"Missing Value Counts\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Histogram of Missing Value Counts In Feature Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large datasets the analysis can run out of memory, or hit recursion depth constraints; \n",
    "# especially when doing correlation analysis on large free text fields\n",
    "pandas_profiling.ProfileReport(hcc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('initial header list')\n",
    "old_header = hcc_df.columns\n",
    "print(old_header)\n",
    "\n",
    "# conver the headers to lower case\n",
    "hcc_df.columns = [x.lower() for x in hcc_df.columns]\n",
    "\n",
    "# replace spaces\n",
    "hcc_df.columns = [x.replace(\" \", \"_\") for x in hcc_df.columns]\n",
    "\n",
    "# remove units\n",
    "hcc_df.columns = [re.sub(r\"_*\\(.*\\)\",\"\", x) for x in hcc_df.columns]\n",
    "\n",
    "# remove special characters if located at the end of the string i.e. * in international_normalised_ratio* or _ in symptoms_\n",
    "hcc_df.columns = [re.sub(r\"[^a-zA-Z0-9]$\",\"\",x) for x in hcc_df.columns]\n",
    "\n",
    "print('')\n",
    "print('resulting header list')\n",
    "print(hcc_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with the information of \"class_attribute\" called target\n",
    "hcc_df['target'] = hcc_df['class_attribute']\n",
    "hcc_df.drop('class_attribute', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows\n",
    "* remove any rows that have a missing outcome variable value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Columns\n",
    "* Remove unneeded or clearly irrelevant columns such as instance id. \n",
    "* Prevent [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/)\n",
    "    * Remove precursor features (used to build outcome variable in study)\n",
    "    * Remove features that would be unavailable when prediction made.\n",
    "We have reviewed the data dictionary for our target dataset and have found no columns that need to be removed for this analysis. This will be skipped. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Missing Data\n",
    "\n",
    "<img src=\"images/missingness.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include statistics of the number of continuous and categorical variables\n",
    "for cols in hcc_df.columns:\n",
    "    # Median-Value Imputation (For continuous-valued features)\n",
    "    if hcc_df[cols].nunique() > 10: #10 chosen as a convenient cutoff for discriminating discrete from continuous variables. \n",
    "        hcc_df[cols].fillna(hcc_df[cols].median(), inplace=True)\n",
    "    # Mode-Value Imputation (For discrete-valued features)\n",
    "    else:\n",
    "        hcc_df[cols].fillna(hcc_df[cols].mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the number of unique values for each variable/feature. \n",
    "unique_count = hcc_df.nunique()\n",
    "unique_count\n",
    "\n",
    "#Re-evaluate missingness and data availability\n",
    "print(\"Missing Value Counts\")\n",
    "missing_count = hcc_df.isnull().sum()\n",
    "missing_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on the data dictionary a number of variables have been assigned the wrong data type by Pandas.  We will remedy this below. \n",
    "\n",
    "* Before moving forward we will first recode our class variable follow the more conventional data standard of the 'positive' class being the minority class.  This will help with downstream evaluation interpretation. We are effectively rephrasing the predictive goal for this data to 'predicting the target event of a patient dying' which represents the miniority class. From here on out, class 0 = survived 1 year while class 1 = died. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously Class Attribute\n",
    "outcome_name = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recode class values (0's to 1's and 1's to 0's) \n",
    "hcc_df[outcome_name]=hcc_df[outcome_name].replace(to_replace=0, value=2)\n",
    "hcc_df[outcome_name]=hcc_df[outcome_name].replace(to_replace=1, value=0)\n",
    "hcc_df[outcome_name]=hcc_df[outcome_name].replace(to_replace=2, value=1) \n",
    "\n",
    "# # similar logic\n",
    "# hcc_df['target'] = ~ hcc_df['target'].astype(bool)\n",
    "\n",
    "#Grab column names as a list\n",
    "header = list(hcc_df)\n",
    "\n",
    "#Cast variable types as needed. It is useful here to specifiy categorical variables here as 'object' for the exploratory analysis.\n",
    "hcc_df[header[0:23]] = hcc_df[header[0:23]].astype(dtype='object')\n",
    "hcc_df[['age_at_diagnosis']] = hcc_df[['age_at_diagnosis']].astype(dtype='float64')\n",
    "hcc_df[['performance_status']] = hcc_df[['performance_status']].astype(dtype='float64')\n",
    "hcc_df[[outcome_name]] = hcc_df[[outcome_name]].astype(dtype='object')\n",
    "\n",
    "#Confirm correct casting of variable types. \n",
    "hcc_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "Determine what magnitude (if any) of [class imbalance](http://www.chioka.in/class-imbalance-problem/) exists in this dataset. Classes are considered to be 'balanced' if there are an equal number of instances within each class.  Class imbalance can be accounted for by applying the proper evaluation metrics downstream.  Generally speaking machine learning methods are most successful when training on more 'balanced' datasets.  Datasets can be artificially proprocessed to be more balanced using [oversampling and undersampling](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis) methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Counts of each class\")\n",
    "hcc_df[outcome_name].value_counts()\n",
    "hcc_df[outcome_name].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Counts (Checking for Imbalance)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We observe some class imbalance (1:1.61) where there are more 1's than 0's, where 1=lived, and 0=died.\n",
    "* Goal = 'predicting the target event of a patient dying'\n",
    "* class 0 = survived 1 year, class 1 = died. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "Our knowledge of the dataset and it's target domain should be applied to perform a manual quality control check of the data.  Specifically do we observe any [outliers](https://en.wikipedia.org/wiki/Outlier) in any of the variables of the dataset? For instance we might check the 'Age at Diagnosis' variable and confirm that we don't see any ages outside of what would be reasonable for this target study (e.g. it's highly unlikely to observe anyone over the age of 110). Obvious highly unlikely or impossible outliers should be removed (i.e. either treated as a missing value, or the entire instance removed). These are often typos during data entry. \n",
    "\n",
    "* We don't observe any obvious 'impossible' outliers that need to be removed, however some statistical outliers are observed in the boxplots and in the basic descriptive summary statistics above.  For the purposes of this analysis we will not remove any of these outliers, however this is a consideration for followup analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Partitioning\n",
    "In order to rigourously evaluate our downstream predictive modeling it is important to partition our entire datasets (at minimum) into a training as well as a testing dataset. Even better if you have a sufficient number of instances in your dataset, it may be split into [training, validation, and testing sets](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7). Testing data will not be applied until modeling is complete in order to evaluate our model on data it has not yet seen.  This is critical to properly evaluating predictive success. \n",
    "\n",
    "<img src=\"images/training_validation_testing.png\" />\n",
    "\n",
    "For the purposes of notebook simplicity we will employ a single basic training/testing partition of the dataset. \n",
    "\n",
    "<img src=\"images/train_test.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the purposes of downstream ML recast all features previously assigned to be 'object' to 'int'\n",
    "hcc_df[header[0:23]] = hcc_df[header[0:23]].astype(dtype='int')\n",
    "hcc_df[['target']] = hcc_df[['target']].astype(dtype='int')\n",
    "# hcc_df.dtypes\n",
    "#Partition data into a training and testing set using convenient scikit-learn command.\n",
    "train, test = train_test_split(hcc_df, test_size=0.2, stratify=hcc_df[outcome_name],random_state=randSeed) # 20% test \n",
    "# set size, stratify option used to ensure class ratio is maintained in the partitions, random seed specified for reproducibility\n",
    "print('Confirm dimensions of training set')\n",
    "print(train.shape)\n",
    "print('Confirm dimensions of testing set')\n",
    "print(test.shape) #Confirm dimensions of testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We confirm below that both the training and test sets have preserved the original class balance to help avoid this additional bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts of each class in training data\")\n",
    "train[outcome_name].value_counts()\n",
    "train[outcome_name].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Counts (Checking for Imbalance)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts of each class in testing data\")\n",
    "test[outcome_name].value_counts()\n",
    "test[outcome_name].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Counts (Checking for Imbalance)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataframe into features and outcome (standard format for scikit-learn methods)\n",
    "x_train = train.drop(outcome_name, axis=1).values\n",
    "y_train = train[outcome_name].values\n",
    "\n",
    "#Later when evaluating models we will need y_test so we will create it now...\n",
    "y_test = test[outcome_name].values\n",
    "x_test = test.drop(outcome_name, axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<img src=\"images/modeling.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# MODELING\n",
    "This is the stage that everyone thinks of when it comes to machine learning and data mining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Selection\n",
    "As a general rule of thumb it is typically best practice to run a number of machine learning algorithms in modeling (i.e. at least 2-3), and ideally those methods should have differen strengths and weaknesses.  This is due to the [no-free-lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) that essentially suggests that no one method can perform ideally in all circumstances.  Therefore, when we are faced with the analysis of some new dataset there is no way to know the optimal strategy to apply ahead of time. Sometimes simple analysis methods can work optimally (as well as run quickly with interpretable results), however other times much more complicated methods are required that may have different trade-offs. \n",
    "\n",
    "<img src=\"images/ml_algorithms.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Decision Tree algorithm\n",
    "\n",
    "In this analysis/notebook we have decided to focus on [decision trees](https://en.wikipedia.org/wiki/Decision_tree). We have selected decision trees because:\n",
    "- Easy to understand and an intuitive ML modeling approach\n",
    "- The models produced by these systems are easy to interpret and apply\n",
    "- Can perform both classification and regression task (CART)\n",
    "- Capable of fitting complex data\n",
    "- Very little data preparation (do not require feature scaling or centering at all)\n",
    "- Decision trees also provide the foundation for more advanced ensemble methods suc has Random Forest, Adaptive Boosting (AdaBoost), Gradient Boosting Machine (XGBoost)\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- Decision trees are often suseptible to bias, and even minor instance sampling differences in the training set can alter the trained model, impacting model generalizability and reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Modeling\n",
    "\n",
    "<img src=\"images/dt_terminology.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees Training\n",
    "\n",
    "- Beging at 'root' node\n",
    "- Recursevly finds a variable that best divides data into outcomes. __Hunt's Algorithm__\n",
    "- ‘best’ variable is determined heuristically\n",
    "    - CART (Classification and Regression Trees) → uses Gini Index(Classification) as metric\n",
    "- Heuristics: produce splits as homogeneous (pure) as possible in terms of outcome labels\n",
    "- Stop criterion: Max depth or purity of labels in each leaf\n",
    "    - helps to prevent overfitting \n",
    "    \n",
    "### Decision Trees Splitting\n",
    "\n",
    "\n",
    "<img src=\"images/dt_splitting.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree: Fitting with Splits (examples)\n",
    "\n",
    "- case = green, control = blue\n",
    "- 2 variables = X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/firstlevel.png\" />\n",
    "<img src=\"images/secondlevel.png\" />\n",
    "<img src=\"images/thirdlevel.png\" />\n",
    "<img src=\"images/fourthlevel.png\" />\n",
    "<img src=\"images/fifthlevel.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Rule "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Gini index is the name of the cost function used to evaluate splits in the dataset\n",
    "- A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split\n",
    "- A perfect separation results in a Gini score of 0, whereas the worst case split that results in 50/50 classes in each group result in a Gini score of 0.5 (for a 2 class problem)\n",
    "\n",
    "<img src=\"images/gini.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn package is used to implement a decision tree algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree model. (no hyperparameters set other than random seed, i.e. default hyperparameters applied)\n",
    "dt = tree.DecisionTreeClassifier(random_state=randSeed)\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dt.fit(x_train, y_train)\n",
    "\n",
    "# Determine model's predictions on training data\n",
    "train_pred = dt.predict(x_train)\n",
    "print(\"Training Accuracy:\",metrics.accuracy_score(y_train, train_pred))\n",
    "\n",
    "# Determine model's predictions on testing data\n",
    "test_pred = dt.predict(x_test)\n",
    "print(\"Testing Accuracy:\",metrics.accuracy_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using a basic accuracy metric (i.e. (TP+TN)/(TP+TN+FP+FN)) our decision tree yields a training accuracy and a testing accuracy.  Keep in mind that our dataset is imbalanced so this evaluation metric is not optimal and our interpretation may be biased. We will revist this below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/underfitting_overfitting.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Sweep\n",
    "Most any ML method has 'hyper' or 'run' parameters that the user can tune/adjust, modifying how the algorithm functions. The process of tweaking and optimizing these parameters to improve ML performance is known as [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization). A hyperparameter sweep mostly commonly includes idenifying which algorithm parameters are known to, or are likely to, impact performance on a given dataset, and then varying the setting of these one or more paramters in separate runs of the algorithm.  This step is not only important for squeezing the maximum performance out of ML methods, but also in fairly compairing the performance of different ML methods on given datasets.  \n",
    "\n",
    "Out of necessity, almost all ML algorithms have default hyperparameters specified (related to [default arguments](https://en.wikipedia.org/wiki/Default_argument). First and foremost, default parameters are specified so that the algorithm is able to run even if the user forgets to specify these hyperparamters. They are often set with the intention of testing out the method on a 'toy' or demonstration dataset included with the software, or they are set as simple, accessible placeholders to ensure that the first time a user runs the software it runs smoothly and quickly. However default parameters are certainly not guarenteed (if even likely) to lead to optimal ML performance on a given dataset. \n",
    "\n",
    "The most common and basic hyperparameter sweep strategies include a [grid search](https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998) or a [random search](https://en.wikipedia.org/wiki/Random_search), but other more sophisticated searches are also employed. \n",
    "\n",
    "Like with many other things in ML, there is no absolute right way to perform a hyperparameter sweep. One could easily waste a large amount of computational time on an exhaustive serach of hyperparameter settings and combinations without improving performance much, or at all. However some degree of hyperparameter searching outside of using the default hyperparameter settings is considered essential to ML best practices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - Random Sweep of Major Hyperparameters\n",
    "\n",
    "<img src=\"images/hyper.png\" />\n",
    "\n",
    "We employ a simple randomized hyperparameter search (with built in, internal cross validation - based on our training sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare a range/set of hyperparameter values for each\n",
    "param_grid = {\"max_depth\": [3, 4, 5, 6, 7, 9, 10, None], \"min_samples_split\": randint(2, 100), \"min_samples_leaf\": randint(1, 100), \n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "model = tree.DecisionTreeClassifier(random_state=randSeed)\n",
    "#Specifics of the random sweep - up to 100 randomly selected hyperparameter combinations\n",
    "hp_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, random_state=randSeed)\n",
    "hp_search.fit(x_train, y_train)\n",
    "\n",
    "# summarize the results of the random parameter search\n",
    "print(\"**************************\")\n",
    "print(hp_search.best_score_)\n",
    "print(\"************************************************************************************\")\n",
    "print(\"The model with the best accuracy was built with the following hyperparameter values.\")\n",
    "print(hp_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We retrain a decision tree on our training data using these 'optimal' parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new decision tree model. (hyperparameters set to those that yielded the best accuracy in the random sweep)\n",
    "dt = tree.DecisionTreeClassifier(max_depth=hp_search.best_params_['max_depth'], \n",
    "                                 min_samples_leaf=hp_search.best_params_['min_samples_leaf'], \n",
    "                                 min_samples_split=hp_search.best_params_['min_samples_split'], \n",
    "                                 criterion=hp_search.best_params_['criterion'],random_state=randSeed)\n",
    "dt = dt.fit(x_train, y_train)\n",
    "\n",
    "# Determine model's predictions on training data\n",
    "dt_train_pred = dt.predict(x_train)\n",
    "print(\"Training Accuracy:\",metrics.accuracy_score(y_train, dt_train_pred))\n",
    "\n",
    "# Determine model's predictions on testing data\n",
    "dt_test_pred = dt.predict(x_test)\n",
    "print(\"Testing Accuracy:\",metrics.accuracy_score(y_test, dt_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that we have trained models, how do we best evaluate and compare the performance of these models?  Proper evaluation is critical to making good conclusions. Many metrics and measures of model 'goodness' exist. Part of what differentiates these evaluation methods is understanding the assumptions being made by a given metric, or what the metric prioritizes as important for model goodness. A nice review of key evaluation metrics can be found [here](http://www.davidsbatista.net/blog/2018/08/19/NLP_Metrics/). \n",
    "\n",
    "Below we will apply some different evaluation methods to the decision tree model that was trained using the 'optimal' parameters settings indentified in the random sweeep (we will leave out the random forest evaluation here for brevity). All evaluations will focus on predictive performance on the testing data. \n",
    "\n",
    "The metrics are typically based on calculations using the different possible classification predictions that can be made: True Positives (TP), True Negative (TN), False Positive (FP), and False Negative (FN).  See [here](https://towardsdatascience.com/the-mystery-of-true-positive-true-negative-false-positive-and-false-negative-fd73c78c905a) to review TP, TN, FP, FN.\n",
    "\n",
    "Let's start by calculating each of these for our test data using our 'optimized' decision tree model. Recall that our dataset is imbalanced and there are more 'negatives' than 'positives'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting confusion matrix\n",
    "def plot_confusion_matrix(y_test, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    # For unique lebel read this:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.utils.multiclass.unique_labels.html#sklearn.utils.multiclass.unique_labels\n",
    "    \n",
    "    classes = unique_labels(y_test, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate confusion matrix (true outcomes, predicted outcomes)\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, dt_test_pred).ravel()\n",
    "\n",
    "print(\"TP = {0}\".format(TP))\n",
    "print(\"FP = {0}\".format(FP))\n",
    "print(\"FN = {0}\".format(FN))\n",
    "print(\"TN = {0}\".format(TN))\n",
    "\n",
    "plot_confusion_matrix(y_test, dt_test_pred, classes=np.unique(y_train), title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values (TP,FP,FN,TN) are components used to calculate a number of different evaluation metrics for classification tasks:\n",
    "\n",
    "* Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "* Precision (a.k.a. Sensitivity)= TP/(TP+FP)\n",
    "* Recall (a.k.a. True Positive Rate) = TP/(TP+FN)\n",
    "* Specificity = TN/(TN+FP)\n",
    "* False Positive Rate = FP/(FP+TN)\n",
    "\n",
    "* F1 Score = 2*(Precision * Recall)/(Precision + Recall)\n",
    "* Balanced Accuracy = (Sensitivity + Specificity)/2\n",
    "\n",
    "We will calculate these for our 'optimized' decision tree model.  Be aware that a different set of evaluation metrics would be used for continuous-valued outcomes (i.e. regression tasks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine model's predictions on testing data\n",
    "print(\"All metrics report performance on testing data!\")\n",
    "print(\"***********************************************\")\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, dt_test_pred))\n",
    "print(\"***********************************************\")\n",
    "\n",
    "#Generate a classification report\n",
    "report = classification_report(y_test, dt_test_pred)\n",
    "print (report)\n",
    "print(\"***********************************************\")\n",
    "specificity = TN/(TN+FP)\n",
    "sensitivity = TP/(TP+FP)\n",
    "print(\"Specificity: \"+str(specificity))\n",
    "print(\"***********************************************\")\n",
    "print(\"False Positive Rate: \"+str(FP/(TN+FP))+ \"   Note: Lower is better.\")\n",
    "print(\"***********************************************\")\n",
    "print(\"Balanced Accuracy: \"+str((sensitivity+specificity)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For imbalanced datasets, balanced accuracy is less biased in comparing performance as it weights the accurate prediction of both classes equally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve and AUC metric\n",
    "The ROC curve is a metric used for the evaluation of binary classification models, where we evaluate how the model would perform if different cuttoff thresholds (of predicted class probability) between the two classes were considered. This is useful when we don't necessarily know which class is more important to predict accurately. For example, if we wanted to evaluate how well a test for a highly infectious disease worked, we would care much more about identifying positive individuals correctly than negative individuals, even at the expense of additional false positives.\n",
    "\n",
    "To calculate an ROC curve first we need to get the class prediction probabilites from the model, rather than just the predicted classes, since these predicted classes were decided based on a predetermined classification threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Determine probabilities of class predictions for each test instance (this will be used much later in calculating an ROC curve)\n",
    "probas_ = dt.fit(x_train, y_train).predict_proba(x_test)\n",
    "\n",
    "# Compute ROC curve and area the curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#Plot the ROC Curve and include AUC in figure. \n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSTPROCESSING\n",
    "Here we discuss some of the last steps in the machine learning analysis pipeline geared towards understanding our model, drawing conclusions, deriving signifiance statistics on our results, or preparing our predictive model for deployment. Here we highlight some of the basics of interpreting our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Decision Tree Model\n",
    "Generate a visualization of the decision tree showing all nodes, splits, and leaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(dt, out_file=None,feature_names=hcc_df.drop('target', axis=1).columns, class_names=['0','1']) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"decisionTree\") \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive Rules from Decision Tree\n",
    "We can also seek to extract human readible rule expressions from any branch paths from the root node (top) of the tree down to any given leaf node (bottom). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_code(tree, feature_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    #print(\"def tree({}):\".format(\", \".join(feature_names)))\n",
    "\n",
    "    def recurse(node, depth):\n",
    "        indent = \"  \" * depth\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            print(\"{}if {} <= {}:\".format(indent, name, threshold))\n",
    "            recurse(tree_.children_left[node], depth + 1)\n",
    "            print(\"{}else:  # if {} > {}\".format(indent, name, threshold))\n",
    "            recurse(tree_.children_right[node], depth + 1)\n",
    "        else:\n",
    "            print(\"{}return {}\".format(indent, tree_.value[node]))\n",
    "\n",
    "    recurse(0, 1)\n",
    "\n",
    "tree_to_code(dt,hcc_df.drop('target', axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Scores - Random Forest\n",
    "Here we return to our random forest model to examine a commonly used output to help with model interpretation, i.e. feature importance scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = dt.feature_importances_\n",
    "names_importances = {'Names':hcc_df.drop('target', axis=1).columns, 'Scores':importances} \n",
    "ni = pd.DataFrame(names_importances)\n",
    "ni = ni.sort_values(by='Scores')\n",
    "ni.shape\n",
    "ni #Report sorted feature scores\n",
    "\n",
    "#Visualize sorted feature scores\n",
    "ni['Scores'].plot(kind='barh',figsize=(6,12))\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.yticks(np.arange(len(hcc_df.drop('target', axis=1).columns)), ni['Names'])\n",
    "plt.title('Sorted Feature Importance Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<img src=\"images/feature_processing.png\" />\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Processing\n",
    "\n",
    " * [Feature Selection](https://en.wikipedia.org/wiki/Feature_selection) - The process of identifying a subset of 'relevant' and sometimes 'non-redundant' features to pass on to modeling.  Ideally feature selection will preserve all relevant features and eliminate all irrelevant or completely redundant features (however this is non-trivial). Three families of feature selection algorithms are typically described: filter-based methods, wrapper methods, and embedded methods. Notably, filter-based feature selection algorithms also serve as feature-weighting algorithms that can be employed as part of the exploratory analysis to assess which features are most likely to be informative. \n",
    "    * When applying filter-based feature selection it is generally up to the data scientist to decide how many variables to select, and how many to remove.  In datasets with very large numbers of features, it is usually a practical necessity to reduce the feature space as much as possible.  Different feature selection methods may offer some guidelines for when there is no evidence that a given feature is relevant (and thus may be removed from consideration). However, one must be aware of the inherant limitations of a given feature selection method before deciding whether to remove a feature, based on the goals or scope of the desired analysis (e.g. just because a feature selection method that can detect univariate associations suggests that a feature is irrelvant may not mean that a method that can take complex associations into account may identify it as being relevant). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SelectFromModel__ is a meta-transformer that can be used along with any estimator that has a coef_ or feature_importances_ attribute after fitting. The features are considered unimportant and removed, if the corresponding coef_ or feature_importances_ values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(random_state=randSeed)\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "# obtain the relevant/most important features\n",
    "model = SelectFromModel(dt, prefit=True)\n",
    "# obtain the names of the important features\n",
    "important_features = hcc_df.drop('target', axis=1).columns[model.get_support()]\n",
    "# create a new set of datasets with only the new features\n",
    "new_x_train = model.transform(x_train)\n",
    "new_x_test = model.transform(x_test)\n",
    "\n",
    "print('old x_train shape = {0}'.format(x_train.shape))\n",
    "print('new x_train shape = {0}'.format(new_x_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new decission tree object\n",
    "new_dt = dt\n",
    "# train it\n",
    "new_dt.fit(new_x_train, y_train)\n",
    "\n",
    "# Determine model's predictions on training data\n",
    "train_pred = new_dt.predict(new_x_train)\n",
    "print(\"Training Accuracy:\",metrics.accuracy_score(y_train, train_pred))\n",
    "\n",
    "# Determine model's predictions on testing data\n",
    "test_pred = new_dt.predict(new_x_test)\n",
    "print(\"Testing Accuracy:\",metrics.accuracy_score(y_test, test_pred))\n",
    "\n",
    "# Prepare a range/set of hyperparameter values for each\n",
    "param_grid = {\"max_depth\": [3, 4, 5, 6, 7, 9, 10, None], \"min_samples_split\": randint(2, 100), \"min_samples_leaf\": randint(1, 100), \n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "model = tree.DecisionTreeClassifier(random_state=randSeed)\n",
    "#Specifics of the random sweep - up to 100 randomly selected hyperparameter combinations\n",
    "hp_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, random_state=randSeed)\n",
    "hp_search.fit(new_x_train, y_train)\n",
    "\n",
    "# summarize the results of the random parameter search\n",
    "print(\"**************************\")\n",
    "print(hp_search.best_score_)\n",
    "print(\"************************************************************************************\")\n",
    "print(\"The model with the best accuracy was built with the following hyperparameter values.\")\n",
    "print(hp_search.best_params_)\n",
    "\n",
    "# Train a new decision tree model. (hyperparameters set to those that yielded the best accuracy in the random sweep)\n",
    "new_dt = tree.DecisionTreeClassifier(max_depth=hp_search.best_params_['max_depth'], \n",
    "                                 min_samples_leaf=hp_search.best_params_['min_samples_leaf'], \n",
    "                                 min_samples_split=hp_search.best_params_['min_samples_split'], \n",
    "                                 criterion=hp_search.best_params_['criterion'],random_state=randSeed)\n",
    "new_dt.fit(new_x_train, y_train)\n",
    "\n",
    "# Determine model's predictions on training data\n",
    "dt_train_pred = new_dt.predict(new_x_train)\n",
    "print(\"Training Accuracy:\",metrics.accuracy_score(y_train, dt_train_pred))\n",
    "\n",
    "# Determine model's predictions on testing data\n",
    "dt_test_pred = new_dt.predict(new_x_test)\n",
    "print(\"Testing Accuracy:\",metrics.accuracy_score(y_test, dt_test_pred))\n",
    "\n",
    "plot_confusion_matrix(y_test, dt_test_pred, classes=np.unique(y_train), title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize it\n",
    "dot_data = tree.export_graphviz(new_dt, out_file=None,feature_names=important_features, class_names=['0','1']) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"decisionTree\") \n",
    "graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
