{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was provided by Dr. Ryan Urbanowicz to be used during the Math, Engineering and Technology at CHOP (METACHOP) Workshop on June 19th, 2019.\n",
    "\n",
    "*** \n",
    "# METACHOP Workshop\n",
    "* Contributors: Jorge Guerra, Helen Loeb and Remo Williams\n",
    "*  Affiliation: Children's Hospital of Philadelphia\n",
    "* Date: 05/20/2019\n",
    "* github: https://github.com/guerrajorge/metachop\n",
    "\n",
    "***\n",
    "# Machine Learning (ML) 102 Workshop\n",
    "* Author: Ryan Urbanowicz, PhD (Debugging assistance by Dr. Trang Le)\n",
    "*  Affiliation: University of Pennsylvania - Department of Biostatistics, Epidemiology, and Informatics & Institute for Biomedical Informatics (IBI) in collaboration with the Leonard Davis Institute (LDI)\n",
    "* Date: 5/1/19\n",
    "* github: https://github.com/UrbsLab/ML_Pipeline_Notebooks\n",
    "\n",
    "A recording of Machine learning 101 Workshop (Dec 2018) – An Introduction to Machine Learning is available at:  \n",
    "https://bluejeans.com/playback/s/PF3d7xdm3DSBbZgHw6JpHnhoVSPVg2ACytbA6eMKFHRWEXSV2UaFNHMXJn7GV9kN\n",
    "\n",
    "A recording of Machine Learning 102 Workshop (May 2019) – Machine Learning: An Analysis Pipeline is available at:\n",
    "https://bluejeans.com/playback/s/sYL8Nfeq9M1H42nLcGPxuxc59aj1DZI6o3Qf8EYnApXP1W2vnphICfuuxlsokPIF\n",
    "\n",
    "To easily view this Jupyter Notebook in Google collab, use the following link: \n",
    "https://colab.research.google.com/github/UrbsLab/ML_Pipeline_Notebooks/blob/master/ML_102_Workshop.ipynb\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/python_jupyter.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Introduction\n",
    "This notebook presents an example of a machine learning analysis pipeline from start to finish. It was written to be paired with the ML 102 Workshop presented in collaboration with IBI and LDI, and it was modified for the CHOP METACHOP workshop. Please note that this notebook is meant to present an accessible example, but does not necessarily include the optimal strategies to analyze the target dataset examined herein. Identifying the optimal analysis pipeling steps/components is one of the fundamental challenges of data science.  This is almost never known ahead of time when seeking to tackle a new dataset/anlaysis. The pipeline presented below could be reproduced using different software or coding languages.  We have opted to utilize Python and the Jupyter notebook framework here due to is accessibility, flexibility, and prevalence in the ML community. \n",
    "\n",
    "For the purposes of this workshop this notebook is primarily meant to be viewed as an html link (pre-run) as a reference/resource/example.  This avoids the many possible complications related to installing python, the necessary packages, understanding how to use/run jupyter notebooks, etc.  However, both the html and original jupyter notebook file are available for those who wish to try out the notebooks themselves.  However explaining how to code in python or the logistics of how to run this notebook interactively are beyond the intended scope of the workshop itself ??\n",
    "\n",
    "<img src=\"images/ds_pipeline.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The following warning will be displayed when trying to run the notebook:\n",
    "\n",
    "<img src=\"images/authorization.png\" />\n",
    "\n",
    "Unselect __\"Reset all runtimes before running\"__ and Click on __\"RUN ANYWAY\"__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to install the following packages (not naitive to google colab)\n",
    "#!pip install skrebate\n",
    "\n",
    "#Basic Packages\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from scipy.stats import randint\n",
    "import re\n",
    "\n",
    "#Scikit-Learn Packages:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif # Mutual information for a discrete target.\n",
    "from sklearn import tree #import decision tree package\n",
    "from sklearn import metrics #import evaluation metric package\n",
    "from sklearn.ensemble import RandomForestClassifier #import decision tree package\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import metrics\n",
    "\n",
    "# In-depth data exploration package\n",
    "import pandas_profiling\n",
    "\n",
    "\n",
    "#ReliefF feature selection package\n",
    "from skrebate import ReliefF\n",
    "\n",
    "#Visualization Packages:\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline   \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import _tree\n",
    "from matplotlib import pyplot\n",
    "import graphviz \n",
    "\n",
    "# Jupyter Notebook Hack: This code ensures that the results of multiple commands within a given cell are all displayed, rather than just the last. \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#Set a random seed for the notebook so that individual runs of the notebook yield the same results\n",
    "randSeed = 99 #changing this value will potentially change the models and results due to stochastic elements of the pipeline. \n",
    "np.random.seed(randSeed)\n",
    "\n",
    "#After running this cell, any error message here will inform you what package still needs to be installed on your system using pip install or conda install."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# RAW DATA\n",
    "Here we describe our target dataset, load it, and examine some basic properties of the data.  This examination of the data can be considered part of the exploratory analysis.  We have included it in this first section to provide a more logical flow to this analysis pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Description of Raw Target Dataset\n",
    "\n",
    "\n",
    "For the purpose of this notebook we have selected an accessible open source dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). Specifically we will apply our ML pipeline to classification dataset gathered to try and predict one year survival in patients with hepatocellular carcinoma (HCC). [Target Dataset Source](http://archive.ics.uci.edu/ml/datasets/HCC+Survival).\n",
    "\n",
    "<img src=\"images/hcc_dataset_description.png\" />\n",
    "\n",
    "* OpenSource: https://archive.ics.uci.edu/ml/datasets/HCC+Survival\n",
    "\n",
    "Prior to loading the data here we opened the dataset in excel as well as in a text editor noting the following about the dataset: \n",
    "1. The data has comma separated values (i.e. csv format).\n",
    "2. There is no header (i.e. column labels) in the data.\n",
    "3. A secondary data dictionary file is available that describes the features and includes the header values\n",
    "4. Missing values are denoted with '?'\n",
    "5. From the dictionary file we know that the class/outcome column is named 'Class Attribute'.\n",
    "6. Oddly the minority class is coded as 0 (patient died), and the majority class is coded as 1 (patient alive).  This is because the 'target' event in this data is 'patient survived 1 year'. \n",
    "\n",
    "We have created a csv file of the header names in excel taken directly from this data dictionary.  GitHub links to the data files needed for this notebook are included below:\n",
    "\n",
    "* [Header File](https://raw.githubusercontent.com/guerrajorge/METACHOP/master/HCC_headers.txt)\n",
    "* [Data File](https://raw.githubusercontent.com/guerrajorge/METACHOP/master/hcc-data.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (rows, columns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Gender',\n",
       " 'Symptoms ',\n",
       " 'Alcohol',\n",
       " 'Hepatitis B Surface Antigen',\n",
       " 'Hepatitis B e Antigen']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load header names file\n",
    "# use the 'raw location' url to access the files from github \n",
    "header_file = 'https://raw.githubusercontent.com/guerrajorge/METACHOP/master/data/HCC_headers.txt'\n",
    "headers = pd.read_csv(header_file, sep='\\t',header=None) \n",
    "print('Number of (rows, columns)')\n",
    "headers.shape \n",
    "print('First 5 columns:')\n",
    "headers.values[0,:5].tolist()\n",
    "header_list = headers.values[0].tolist() #creates a list variable from the first element dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of (rows, columns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(165, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset and provide header names from above.\n",
    "target_data_file = 'https://raw.githubusercontent.com/guerrajorge/METACHOP/master/data/hcc-data.txt'\n",
    "hcc_df = pd.read_csv(target_data_file, na_values='?', header=None, names = header_list) # Data loaded so that blank excell cells are 'NA'\n",
    "print('Number of (rows, columns)')\n",
    "hcc_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 165 instances.\n",
      "Dataset contains 49 features plus 1 class/outcome.\n"
     ]
    }
   ],
   "source": [
    "num_instances = hcc_df.shape[0] # number of examples\n",
    "num_features = hcc_df.shape[1] # number of features\n",
    "\n",
    "print('Dataset contains {0} instances.'.format(num_instances))\n",
    "print('Dataset contains {0} features plus 1 class/outcome.'.format(num_features-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Examine Basic Data Properties\n",
    "Run some basic Pandas commands to examine/confirm dataset properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Symptoms</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Hepatitis B Surface Antigen</th>\n",
       "      <th>Hepatitis B e Antigen</th>\n",
       "      <th>Hepatitis B Core Antibody</th>\n",
       "      <th>Hepatitis C Virus Antibody</th>\n",
       "      <th>Cirrhosis</th>\n",
       "      <th>Endemic Countries</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>...</th>\n",
       "      <th>Alkaline phosphatase (U/L)</th>\n",
       "      <th>Total Proteins (g/dL)</th>\n",
       "      <th>Creatinine (mg/dL)</th>\n",
       "      <th>Number of Nodules</th>\n",
       "      <th>Major dimension of nodule (cm)</th>\n",
       "      <th>Direct Bilirubin (mg/dL)</th>\n",
       "      <th>Iron</th>\n",
       "      <th>Oxygen Saturation (%)</th>\n",
       "      <th>Ferritin (ng/mL)</th>\n",
       "      <th>Class Attribute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>150.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>174.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>109.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Symptoms   Alcohol  Hepatitis B Surface Antigen  \\\n",
       "0       1        0.0        1                          0.0   \n",
       "1       0        NaN        0                          0.0   \n",
       "2       1        0.0        1                          1.0   \n",
       "3       1        1.0        1                          0.0   \n",
       "4       1        1.0        1                          1.0   \n",
       "\n",
       "   Hepatitis B e Antigen  Hepatitis B Core Antibody  \\\n",
       "0                    0.0                        0.0   \n",
       "1                    0.0                        0.0   \n",
       "2                    0.0                        1.0   \n",
       "3                    0.0                        0.0   \n",
       "4                    0.0                        1.0   \n",
       "\n",
       "   Hepatitis C Virus Antibody  Cirrhosis  Endemic Countries  Smoking  ...  \\\n",
       "0                         0.0          1                0.0      1.0  ...   \n",
       "1                         1.0          1                NaN      NaN  ...   \n",
       "2                         0.0          1                0.0      1.0  ...   \n",
       "3                         0.0          1                0.0      1.0  ...   \n",
       "4                         0.0          1                0.0      1.0  ...   \n",
       "\n",
       "   Alkaline phosphatase (U/L)  Total Proteins (g/dL)  Creatinine (mg/dL)  \\\n",
       "0                       150.0                    7.1                0.70   \n",
       "1                         NaN                    NaN                 NaN   \n",
       "2                       109.0                    7.0                2.10   \n",
       "3                       174.0                    8.1                1.11   \n",
       "4                       109.0                    6.9                1.80   \n",
       "\n",
       "   Number of Nodules  Major dimension of nodule (cm)  \\\n",
       "0                1.0                             3.5   \n",
       "1                1.0                             1.8   \n",
       "2                5.0                            13.0   \n",
       "3                2.0                            15.7   \n",
       "4                1.0                             9.0   \n",
       "\n",
       "   Direct Bilirubin (mg/dL)  Iron  Oxygen Saturation (%)  Ferritin (ng/mL)  \\\n",
       "0                       0.5   NaN                    NaN               NaN   \n",
       "1                       NaN   NaN                    NaN               NaN   \n",
       "2                       0.1  28.0                    6.0              16.0   \n",
       "3                       0.2   NaN                    NaN               NaN   \n",
       "4                       NaN  59.0                   15.0              22.0   \n",
       "\n",
       "   Class Attribute  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                0  \n",
       "4                1  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examine the first 5 rows of the loaded data\n",
    "hcc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember class/outcome column is named '__Class Attribute__' coded as 0 (patient died), and  1 (patient alive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 165 entries, 0 to 164\n",
      "Data columns (total 50 columns):\n",
      "Gender                              165 non-null int64\n",
      "Symptoms                            147 non-null float64\n",
      "Alcohol                             165 non-null int64\n",
      "Hepatitis B Surface Antigen         148 non-null float64\n",
      "Hepatitis B e Antigen               126 non-null float64\n",
      "Hepatitis B Core Antibody           141 non-null float64\n",
      "Hepatitis C Virus Antibody          156 non-null float64\n",
      "Cirrhosis                           165 non-null int64\n",
      "Endemic Countries                   126 non-null float64\n",
      "Smoking                             124 non-null float64\n",
      "Diabetes                            162 non-null float64\n",
      "Obesity                             155 non-null float64\n",
      "Hemochromatosis                     142 non-null float64\n",
      "Arterial Hypertension               162 non-null float64\n",
      "Chronic Renal Insufficiency         163 non-null float64\n",
      "Human Immunodeficiency Virus        151 non-null float64\n",
      "Nonalcoholic Steatohepatitis        143 non-null float64\n",
      "Esophageal Varices                  113 non-null float64\n",
      "Splenomegaly                        150 non-null float64\n",
      "Portal Hypertension                 154 non-null float64\n",
      "Portal Vein Thrombosis              162 non-null float64\n",
      "Liver Metastasis                    161 non-null float64\n",
      "Radiological Hallmark               163 non-null float64\n",
      "Age at diagnosis                    165 non-null int64\n",
      "Grams of Alcohol per day            117 non-null float64\n",
      "Packs of cigarets per year          112 non-null float64\n",
      "Performance Status*                 165 non-null int64\n",
      "Encephalopathy degree*              164 non-null float64\n",
      "Ascites degree*                     163 non-null float64\n",
      "International Normalised Ratio*     161 non-null float64\n",
      "Alpha-Fetoprotein (ng/mL)           157 non-null float64\n",
      "Haemoglobin (g/dL)                  162 non-null float64\n",
      "Mean Corpuscular Volume             162 non-null float64\n",
      "Leukocytes(G/L)                     162 non-null float64\n",
      "Platelets                           162 non-null float64\n",
      "Albumin (mg/dL)                     159 non-null float64\n",
      "Total Bilirubin(mg/dL)              160 non-null float64\n",
      "Alanine transaminase (U/L)          161 non-null float64\n",
      "Aspartate transaminase (U/L)        162 non-null float64\n",
      "Gamma glutamyl transferase (U/L)    162 non-null float64\n",
      "Alkaline phosphatase (U/L)          162 non-null float64\n",
      "Total Proteins (g/dL)               154 non-null float64\n",
      "Creatinine (mg/dL)                  158 non-null float64\n",
      "Number of Nodules                   163 non-null float64\n",
      "Major dimension of nodule (cm)      145 non-null float64\n",
      "Direct Bilirubin (mg/dL)            121 non-null float64\n",
      "Iron                                86 non-null float64\n",
      "Oxygen Saturation (%)               85 non-null float64\n",
      "Ferritin (ng/mL)                    85 non-null float64\n",
      "Class Attribute                     165 non-null int64\n",
      "dtypes: float64(44), int64(6)\n",
      "memory usage: 64.5 KB\n"
     ]
    }
   ],
   "source": [
    "# method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage\n",
    "hcc_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Assess Missingness in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Value Counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gender                               0\n",
       "Symptoms                            18\n",
       "Alcohol                              0\n",
       "Hepatitis B Surface Antigen         17\n",
       "Hepatitis B e Antigen               39\n",
       "Hepatitis B Core Antibody           24\n",
       "Hepatitis C Virus Antibody           9\n",
       "Cirrhosis                            0\n",
       "Endemic Countries                   39\n",
       "Smoking                             41\n",
       "Diabetes                             3\n",
       "Obesity                             10\n",
       "Hemochromatosis                     23\n",
       "Arterial Hypertension                3\n",
       "Chronic Renal Insufficiency          2\n",
       "Human Immunodeficiency Virus        14\n",
       "Nonalcoholic Steatohepatitis        22\n",
       "Esophageal Varices                  52\n",
       "Splenomegaly                        15\n",
       "Portal Hypertension                 11\n",
       "Portal Vein Thrombosis               3\n",
       "Liver Metastasis                     4\n",
       "Radiological Hallmark                2\n",
       "Age at diagnosis                     0\n",
       "Grams of Alcohol per day            48\n",
       "Packs of cigarets per year          53\n",
       "Performance Status*                  0\n",
       "Encephalopathy degree*               1\n",
       "Ascites degree*                      2\n",
       "International Normalised Ratio*      4\n",
       "Alpha-Fetoprotein (ng/mL)            8\n",
       "Haemoglobin (g/dL)                   3\n",
       "Mean Corpuscular Volume              3\n",
       "Leukocytes(G/L)                      3\n",
       "Platelets                            3\n",
       "Albumin (mg/dL)                      6\n",
       "Total Bilirubin(mg/dL)               5\n",
       "Alanine transaminase (U/L)           4\n",
       "Aspartate transaminase (U/L)         3\n",
       "Gamma glutamyl transferase (U/L)     3\n",
       "Alkaline phosphatase (U/L)           3\n",
       "Total Proteins (g/dL)               11\n",
       "Creatinine (mg/dL)                   7\n",
       "Number of Nodules                    2\n",
       "Major dimension of nodule (cm)      20\n",
       "Direct Bilirubin (mg/dL)            44\n",
       "Iron                                79\n",
       "Oxygen Saturation (%)               80\n",
       "Ferritin (ng/mL)                    80\n",
       "Class Attribute                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate missingness and data availability\n",
    "print(\"Missing Value Counts\")\n",
    "missing_count = hcc_df.isnull().sum()\n",
    "missing_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We confirm here that there is no missing data in the class/outcome variable. If there had been we would have to remove any rows with missing outcome later in the cleaning section. This is because we are performing supervised learning, i.e.  (label/outcome) required for modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender                                2\n",
       "Symptoms                              2\n",
       "Alcohol                               2\n",
       "Hepatitis B Surface Antigen           2\n",
       "Hepatitis B e Antigen                 2\n",
       "Hepatitis B Core Antibody             2\n",
       "Hepatitis C Virus Antibody            2\n",
       "Cirrhosis                             2\n",
       "Endemic Countries                     2\n",
       "Smoking                               2\n",
       "Diabetes                              2\n",
       "Obesity                               2\n",
       "Hemochromatosis                       2\n",
       "Arterial Hypertension                 2\n",
       "Chronic Renal Insufficiency           2\n",
       "Human Immunodeficiency Virus          2\n",
       "Nonalcoholic Steatohepatitis          2\n",
       "Esophageal Varices                    2\n",
       "Splenomegaly                          2\n",
       "Portal Hypertension                   2\n",
       "Portal Vein Thrombosis                2\n",
       "Liver Metastasis                      2\n",
       "Radiological Hallmark                 2\n",
       "Age at diagnosis                     51\n",
       "Grams of Alcohol per day             19\n",
       "Packs of cigarets per year           30\n",
       "Performance Status*                   5\n",
       "Encephalopathy degree*                3\n",
       "Ascites degree*                       3\n",
       "International Normalised Ratio*      87\n",
       "Alpha-Fetoprotein (ng/mL)           132\n",
       "Haemoglobin (g/dL)                   71\n",
       "Mean Corpuscular Volume             128\n",
       "Leukocytes(G/L)                     105\n",
       "Platelets                           131\n",
       "Albumin (mg/dL)                      41\n",
       "Total Bilirubin(mg/dL)               62\n",
       "Alanine transaminase (U/L)           93\n",
       "Aspartate transaminase (U/L)        107\n",
       "Gamma glutamyl transferase (U/L)    139\n",
       "Alkaline phosphatase (U/L)          124\n",
       "Total Proteins (g/dL)                46\n",
       "Creatinine (mg/dL)                   84\n",
       "Number of Nodules                     6\n",
       "Major dimension of nodule (cm)       68\n",
       "Direct Bilirubin (mg/dL)             41\n",
       "Iron                                 68\n",
       "Oxygen Saturation (%)                57\n",
       "Ferritin (ng/mL)                     84\n",
       "Class Attribute                       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the number of unique values for each variable/feature. Note that missing values are not being include as unique values. \n",
    "unique_count = hcc_df.nunique()\n",
    "unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Unique Value Counts')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of Unique Value Counts In Feature Set')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a histogram of these unique variable counts. \n",
    "ax = unique_count.hist(bins=num_instances,figsize=(16,4))\n",
    "ax.set_xlabel(\"Unique Value Counts\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Histogram of Unique Value Counts In Feature Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We observe that nearly half the features are binary, and there are some features that appear to be discrete interger values, and a number of real-valued features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Missing Value Counts')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of Missing Value Counts In Feature Set')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a histogram of the missingness observed over all features in the dataset\n",
    "ax = missing_count.hist(bins=num_instances,figsize=(16,4))\n",
    "ax.set_xlabel(\"Missing Value Counts\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Histogram of Missing Value Counts In Feature Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side by side comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hcc_unique_missing.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Yet, another package to examine the data\n",
    "\n",
    "Package: pandas_profiling  \n",
    "https://github.com/pandas-profiling/pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pandas_profiling.ProfileReport(hcc_df)\n",
    "profile.to_file('exploratory_plots/hcc_profile_result.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large datasets the analysis can run out of memory, or hit recursion depth constraints; \n",
    "# especially when doing correlation analysis on large free text fields\n",
    "pandas_profiling.ProfileReport(hcc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# PREPROCESSING\n",
    "Every unique dataset and analysis comes with it's own characteristics and challenges.  Therefore preprocessing requires an understanding of the target data and analysis goal and may require fewer, additional, or alternative steps than what we describe here. Also note that an exploratory analysis and data cleaning go hand in hand and are often completed together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing headers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Gender', 'Symptoms ', 'Alcohol', 'Hepatitis B Surface Antigen',\n",
       "       'Hepatitis B e Antigen', 'Hepatitis B Core Antibody',\n",
       "       'Hepatitis C Virus Antibody', 'Cirrhosis', 'Endemic Countries',\n",
       "       'Smoking', 'Diabetes', 'Obesity', 'Hemochromatosis',\n",
       "       'Arterial Hypertension', 'Chronic Renal Insufficiency',\n",
       "       'Human Immunodeficiency Virus', 'Nonalcoholic Steatohepatitis',\n",
       "       'Esophageal Varices', 'Splenomegaly', 'Portal Hypertension',\n",
       "       'Portal Vein Thrombosis', 'Liver Metastasis', 'Radiological Hallmark',\n",
       "       'Age at diagnosis', 'Grams of Alcohol per day',\n",
       "       'Packs of cigarets per year', 'Performance Status*',\n",
       "       'Encephalopathy degree*', 'Ascites degree*',\n",
       "       'International Normalised Ratio*', 'Alpha-Fetoprotein (ng/mL)',\n",
       "       'Haemoglobin (g/dL)', 'Mean Corpuscular Volume', 'Leukocytes(G/L)',\n",
       "       'Platelets', 'Albumin (mg/dL)', 'Total Bilirubin(mg/dL)',\n",
       "       'Alanine transaminase (U/L)', 'Aspartate transaminase (U/L)',\n",
       "       'Gamma glutamyl transferase (U/L)', 'Alkaline phosphatase (U/L)',\n",
       "       'Total Proteins (g/dL)', 'Creatinine (mg/dL)', 'Number of Nodules',\n",
       "       'Major dimension of nodule (cm)', 'Direct Bilirubin (mg/dL)', 'Iron',\n",
       "       'Oxygen Saturation (%)', 'Ferritin (ng/mL)', 'Class Attribute'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hcc_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'symptoms ', 'alcohol', 'hepatitis b surface antigen',\n",
       "       'hepatitis b e antigen', 'hepatitis b core antibody',\n",
       "       'hepatitis c virus antibody', 'cirrhosis', 'endemic countries',\n",
       "       'smoking', 'diabetes', 'obesity', 'hemochromatosis',\n",
       "       'arterial hypertension', 'chronic renal insufficiency',\n",
       "       'human immunodeficiency virus', 'nonalcoholic steatohepatitis',\n",
       "       'esophageal varices', 'splenomegaly', 'portal hypertension',\n",
       "       'portal vein thrombosis', 'liver metastasis', 'radiological hallmark',\n",
       "       'age at diagnosis', 'grams of alcohol per day',\n",
       "       'packs of cigarets per year', 'performance status*',\n",
       "       'encephalopathy degree*', 'ascites degree*',\n",
       "       'international normalised ratio*', 'alpha-fetoprotein (ng/ml)',\n",
       "       'haemoglobin (g/dl)', 'mean corpuscular volume', 'leukocytes(g/l)',\n",
       "       'platelets', 'albumin (mg/dl)', 'total bilirubin(mg/dl)',\n",
       "       'alanine transaminase (u/l)', 'aspartate transaminase (u/l)',\n",
       "       'gamma glutamyl transferase (u/l)', 'alkaline phosphatase (u/l)',\n",
       "       'total proteins (g/dl)', 'creatinine (mg/dl)', 'number of nodules',\n",
       "       'major dimension of nodule (cm)', 'direct bilirubin (mg/dl)', 'iron',\n",
       "       'oxygen saturation (%)', 'ferritin (ng/ml)', 'class attribute'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conver the headers to lower case\n",
    "hcc_df.columns = [x.lower() for x in hcc_df.columns]\n",
    "hcc_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'symptoms_', 'alcohol', 'hepatitis_b_surface_antigen',\n",
       "       'hepatitis_b_e_antigen', 'hepatitis_b_core_antibody',\n",
       "       'hepatitis_c_virus_antibody', 'cirrhosis', 'endemic_countries',\n",
       "       'smoking', 'diabetes', 'obesity', 'hemochromatosis',\n",
       "       'arterial_hypertension', 'chronic_renal_insufficiency',\n",
       "       'human_immunodeficiency_virus', 'nonalcoholic_steatohepatitis',\n",
       "       'esophageal_varices', 'splenomegaly', 'portal_hypertension',\n",
       "       'portal_vein_thrombosis', 'liver_metastasis', 'radiological_hallmark',\n",
       "       'age_at_diagnosis', 'grams_of_alcohol_per_day',\n",
       "       'packs_of_cigarets_per_year', 'performance_status*',\n",
       "       'encephalopathy_degree*', 'ascites_degree*',\n",
       "       'international_normalised_ratio*', 'alpha-fetoprotein_(ng/ml)',\n",
       "       'haemoglobin_(g/dl)', 'mean_corpuscular_volume', 'leukocytes(g/l)',\n",
       "       'platelets', 'albumin_(mg/dl)', 'total_bilirubin(mg/dl)',\n",
       "       'alanine_transaminase_(u/l)', 'aspartate_transaminase_(u/l)',\n",
       "       'gamma_glutamyl_transferase_(u/l)', 'alkaline_phosphatase_(u/l)',\n",
       "       'total_proteins_(g/dl)', 'creatinine_(mg/dl)', 'number_of_nodules',\n",
       "       'major_dimension_of_nodule_(cm)', 'direct_bilirubin_(mg/dl)', 'iron',\n",
       "       'oxygen_saturation_(%)', 'ferritin_(ng/ml)', 'class_attribute'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace spaces\n",
    "hcc_df.columns = [x.replace(\" \", \"_\") for x in hcc_df.columns]\n",
    "hcc_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'symptoms_', 'alcohol', 'hepatitis_b_surface_antigen',\n",
       "       'hepatitis_b_e_antigen', 'hepatitis_b_core_antibody',\n",
       "       'hepatitis_c_virus_antibody', 'cirrhosis', 'endemic_countries',\n",
       "       'smoking', 'diabetes', 'obesity', 'hemochromatosis',\n",
       "       'arterial_hypertension', 'chronic_renal_insufficiency',\n",
       "       'human_immunodeficiency_virus', 'nonalcoholic_steatohepatitis',\n",
       "       'esophageal_varices', 'splenomegaly', 'portal_hypertension',\n",
       "       'portal_vein_thrombosis', 'liver_metastasis', 'radiological_hallmark',\n",
       "       'age_at_diagnosis', 'grams_of_alcohol_per_day',\n",
       "       'packs_of_cigarets_per_year', 'performance_status*',\n",
       "       'encephalopathy_degree*', 'ascites_degree*',\n",
       "       'international_normalised_ratio*', 'alpha-fetoprotein', 'haemoglobin',\n",
       "       'mean_corpuscular_volume', 'leukocytes', 'platelets', 'albumin',\n",
       "       'total_bilirubin', 'alanine_transaminase', 'aspartate_transaminase',\n",
       "       'gamma_glutamyl_transferase', 'alkaline_phosphatase', 'total_proteins',\n",
       "       'creatinine', 'number_of_nodules', 'major_dimension_of_nodule',\n",
       "       'direct_bilirubin', 'iron', 'oxygen_saturation', 'ferritin',\n",
       "       'class_attribute'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove units\n",
    "hcc_df.columns = [re.sub(r\"_*\\(.*\\)\",\"\", x) for x in hcc_df.columns]\n",
    "hcc_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'symptoms_', 'alcohol', 'hepatitis_b_surface_antigen',\n",
       "       'hepatitis_b_e_antigen', 'hepatitis_b_core_antibody',\n",
       "       'hepatitis_c_virus_antibody', 'cirrhosis', 'endemic_countries',\n",
       "       'smoking', 'diabetes', 'obesity', 'hemochromatosis',\n",
       "       'arterial_hypertension', 'chronic_renal_insufficiency',\n",
       "       'human_immunodeficiency_virus', 'nonalcoholic_steatohepatitis',\n",
       "       'esophageal_varices', 'splenomegaly', 'portal_hypertension',\n",
       "       'portal_vein_thrombosis', 'liver_metastasis', 'radiological_hallmark',\n",
       "       'age_at_diagnosis', 'grams_of_alcohol_per_day',\n",
       "       'packs_of_cigarets_per_year', 'performance_status',\n",
       "       'encephalopathy_degree', 'ascites_degree',\n",
       "       'international_normalised_ratio', 'alpha-fetoprotein', 'haemoglobin',\n",
       "       'mean_corpuscular_volume', 'leukocytes', 'platelets', 'albumin',\n",
       "       'total_bilirubin', 'alanine_transaminase', 'aspartate_transaminase',\n",
       "       'gamma_glutamyl_transferase', 'alkaline_phosphatase', 'total_proteins',\n",
       "       'creatinine', 'number_of_nodules', 'major_dimension_of_nodule',\n",
       "       'direct_bilirubin', 'iron', 'oxygen_saturation', 'ferritin',\n",
       "       'class_attribute'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove * characters\n",
    "hcc_df.columns = [x.replace(\"*\", \"\") for x in hcc_df.columns]\n",
    "hcc_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Rows\n",
    "Given that our task is to train a predictive classification model (i.e. supervised learning), remove any rows that have a missing outcome variable value. \n",
    "* In this analysis all rows have values so this can be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Columns\n",
    "Remove unneeded or clearly irrelevant columns such as instance id. Also remove any columns that might contribute to [data leakage](https://machinelearningmastery.com/data-leakage-machine-learning/). E.g. When applying supervised learning to train a predictive model, remove any features that would not be available when it comes time to apply the model to data outside of the training data. \n",
    "* We have reviewed the data dictionary for our target dataset and have found no columns that need to be removed for this analysis. This will be skipped. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Missing Data\n",
    "The first challenge in dealing with missing data is to understand the nature of the [missingness](https://en.wikipedia.org/wiki/Missing_data). Values in a data set can be (1) missing completely at random (MCAR), (2) missing at random (MAR), or missing not at random (MNAR). \n",
    "\n",
    "Many downstream machine learning algorithms and respective implementations are not designed to accomodate the presence of missing value in a dataset (e.g. the popular Python ML package, [scikit-learn](https://scikit-learn.org/stable/), does not allow missing values). Generally there are four main approaches to handle missing data: (1) imputaton - values are added in the place of missing data using one of many proposed strategies, (2) omission - instances and/or features with missing data values are discarded from further analysis, (3) in cases where missingness is not or may not be MCAR, a new varibable can be constructed (i.e. feature construction) that describes if a value was missing or not (i.e. asks the question, is missingness itself informative?), and (4) analysis - by directly applying downstream methods that are unaffected by missing values. \n",
    "\n",
    "* For the purposes of this analysis we will assume that the missing data is MCAR.  Since we will be using skikit-learn to complete our downstream modeling, we can not leave missing values in the dataset. Furthermore, because there is not a particularly large number of instances or features in our data we will avoid ommitting either. Therefore in this analysis we have opted to impute the missing values with a common simple approach, i.e. we will apply **'median-value imputation'** to real-valued features, and **'mode-value imputation'** to discrete features.\n",
    "\n",
    "Note that any employed imputation method is essentially making an 'educated guess' that may be introducing bias.  I recommend that analysis be completed directly using methods that treat missing data 'neutrally' or ignore it, whenever possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in td.columns:\n",
    "    # Median-Value Imputation (For continuous-valued features)\n",
    "    if td[c].nunique() > 10: #10 chosen as a convenient cutoff for discriminating discrete from continuous variables. \n",
    "        td[c].fillna(td[c].median(), inplace=True)\n",
    "    # Mode-Value Imputation (For discrete-valued features)\n",
    "    else:\n",
    "        td[c].fillna(td[c].mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the number of unique values for each variable/feature. \n",
    "unique_count = td.nunique()\n",
    "unique_count\n",
    "\n",
    "#Re-evaluate missingness and data availability\n",
    "print(\"Missing Value Counts\")\n",
    "missing_count = td.isnull().sum()\n",
    "missing_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast Variable Types (as needed)\n",
    "We examine how the variable types are being automatically detected by the Pandas package to either confirm that they are correct or to 'cast' variables so that Pandas and any other applied algorithms will know how to handle each variable when necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "td.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on the data dictionary a number of variables have been assigned the wrong data type by Pandas.  We will remedy this below. \n",
    "\n",
    "* Before moving forward we will first recode our class variable follow the more conventional data standard of the 'positive' class being the minority class.  This will help with downstream evaluation interpretation. We are effectively rephrasing the predictive goal for this data to 'predicting the target event of a patient dying' which represents the miniority class. From her on out, class 0 = survived 1 year while class 1 = died. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recode class values (0's to 1's and 1's to 0's) \n",
    "td[outcome_name]=td[outcome_name].replace(to_replace=0, value=2)\n",
    "td[outcome_name]=td[outcome_name].replace(to_replace=1, value=0)\n",
    "td[outcome_name]=td[outcome_name].replace(to_replace=2, value=1) \n",
    "\n",
    "#Grab column names as a list\n",
    "header = list(td)\n",
    "\n",
    "#Cast variable types as needed. It is useful here to specifiy categorical variables here as 'object' for the exploratory analysis.\n",
    "td[header[0:23]] = td[header[0:23]].astype(dtype='object')\n",
    "td[['Age at diagnosis']] = td[['Age at diagnosis']].astype(dtype='float64')\n",
    "td[['Performance Status*']] = td[['Performance Status*']].astype(dtype='float64')\n",
    "td[['Class Attribute']] = td[['Class Attribute']].astype(dtype='object')\n",
    "\n",
    "#Confirm correct casting of variable types. \n",
    "td.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exploratory/Descriptive Analysis\n",
    "In this section of the notebook we continue our exploratory analysis of the dataset to guide downstream processing and analysis. Here we focus on the following: \n",
    "\n",
    "* Basic summary statistics (real-valued features)\n",
    "* Identify class imbalance\n",
    "* Examing univariate relationships between each feature and outcome with appropriate visualizations\n",
    "    * IF outcome = categorical/discrete and feature = categorical/discrete, THEN use contingency table count bar plot\n",
    "    * IF outcome = categorical/discrete and feature = real/continuous, THEN use boxplot\n",
    "    * IF outcome = real/continuous and feature = real/continuous, THEN use scatterplot\n",
    "    * IF outcome = real/continuous and feature = categorical/discrete, THEN use boxplot\n",
    "* Univariate analysis, i.e. between each feature and outcome (using appropriate test) - We have picked appropriate non-parametric tests here.\n",
    "    * IF outcome = categorical/discrete and feature = categorical/discrete, THEN use Chi Square Test\n",
    "    * IF outcome = categorical/discrete and feature = real/continuous, THEN use Mann-Whitney Test\n",
    "    * IF outcome = real/continuous and feature = real/continuous, THEN use Spearman Correlation\n",
    "    * IF outcome = real/continuous and feature = categorical/discrete, THEN use Mann-Whitney Test\n",
    "* Look for outliers\n",
    "* Other exploratory visualizations might be useful depending on the target problem/data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Summary Statistics\n",
    "Applied to any real-valued features in the dataset.\n",
    "Examine the min and max values in the summary below to identify any obvious (impossible) outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n",
    "Determine what magnitude (if any) of [class imbalance](http://www.chioka.in/class-imbalance-problem/) exists in this dataset. Classes are considered to be 'balanced' if there are an equal number of instances within each class.  Class imbalance can be accounted for by applying the proper evaluation metrics downstream.  Generally speaking machine learning methods are most successful when training on more 'balanced' datasets.  Datasets can be artificially proprocessed to be more balanced using [oversampling and undersampling](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis) methods.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Counts of each class\")\n",
    "td[outcome_name].value_counts()\n",
    "td[outcome_name].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Counts (Checking for Imbalance)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We observe some class imbalance (1:1.61) where there are more 1's than 0's, where 1=lived, and 0=died."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots and Univariate Analysis\n",
    "Below we have encoded a method to automatically select an appropriate plot and univariate association test between a single feature and the target outcome in the dataset.  While our present data only includes discrete/binary outcomes, this method is set up to also handle data with continuous-valued (i.e. regression) outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to automatically select and generate the most appropriate exploratory data visualization pairing each indepentend variable (i.e. feature) with the dependent variable (i.e. outcome) \n",
    "def graph_test_selector(feature_name, outcome_name, data, p_val_dict):\n",
    "\n",
    "    if data[feature_name].dtype == 'object' and data[outcome_name].dtype == 'object': #Feature and Outcome are discrete/categorical/binary\n",
    "        #Generate contingency table count bar plot. ------------------------------------------------------------------------\n",
    "        #Calculate Contingency Table - Counts\n",
    "        print('#################')\n",
    "        table = pd.crosstab(data[feature_name], data[outcome_name])\n",
    "        print(table)\n",
    "        geom_bar_data = pd.DataFrame(table)\n",
    "        mygraph = geom_bar_data.plot(kind='bar')\n",
    "        plt.ylabel('Count')\n",
    "        new_feature_name = feature_name.replace(\" \",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"*\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"/\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        plt.savefig('Exploratory_Plots/BarPlot_'+str(new_feature_name)+'_'+outcome_name+'.pdf')\n",
    "        \n",
    "        #Univariate association test (Chi Square Test of Independence - Non-parametric)\n",
    "        c, p, dof, expected = scs.chi2_contingency(table)\n",
    "        print(\"Chi Square P-value = \"+str(p))\n",
    "        \n",
    "        #Identify and save features with 'significant' univariate association\n",
    "        if p <= 0.05:\n",
    "            p_val_dict[feature_name] = p\n",
    "        \n",
    "        \n",
    "    elif data[feature_name].dtype == 'float64' and data[outcome_name].dtype == 'object': #Feature is continuous and Outcome is discrete/categorical/binary\n",
    "        #Generate boxplot-----------------------------------------------------------------------------------------------------\n",
    "        print('#################')\n",
    "        print(feature_name)\n",
    "        mygraph = data.boxplot(column=feature_name,by=outcome_name)\n",
    "        plt.ylabel(feature_name)\n",
    "        plt.title('')\n",
    "        new_feature_name = feature_name.replace(\" \",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"*\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"/\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        plt.savefig('Exploratory_Plots/BoxPlot_'+str(new_feature_name)+'_'+outcome_name+'.pdf')\n",
    "        \n",
    "        #Univariate association test (Mann-Whitney Test - Non-parametric)\n",
    "        c, p = scs.mannwhitneyu(x=data[feature_name].loc[data[outcome_name] == 0],y=data[feature_name].loc[data[outcome_name] == 1])\n",
    "        print(\"Mann-Whitney P-value = \"+str(p))\n",
    "        print(\"Mann-Whitney U-Statistic = \"+str(c))\n",
    "\n",
    "        #Identify and save features with 'significant' univariate association\n",
    "        if p <= 0.05:\n",
    "            p_val_dict[feature_name] = p\n",
    "        \n",
    "    elif data[feature_name].dtype == 'float64' and data[outcome_name].dtype == 'float64': #Feature is continuous and Outcome is discrete/categorical/binary\n",
    "        #Generate scatterplot-------------------------------------------------------------------------------------------------\n",
    "        print('#################')\n",
    "        print(feature_name)\n",
    "        mygraph = data.plot(x=feature_name,y=outcome_name,kind='scatter')\n",
    "        new_feature_name = feature_name.replace(\" \",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"*\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"/\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        plt.savefig('Exploratory_Plots/ScatterPlot_'+str(new_feature_name)+'_'+outcome_name+'.pdf')\n",
    "\n",
    "        #Univariate association test (Spearman Rank-Order Correlation - Non-parametric)\n",
    "        p = scs.spearmanr(x=data[feature_name],y=data[outcome_name])\n",
    "        print(\"Spearman Correlation P-value = \" + str(p))\n",
    "\n",
    "        #Identify and save features with 'significant' univariate association\n",
    "        if p <= 0.05:\n",
    "            p_val_dict[feature_name] = p        \n",
    "\n",
    "    elif data[feature_name].dtype == 'object' and data[outcome_name].dtype == 'float64': #Feature is continuous and Outcome is discrete/categorical/binary\n",
    "        #Generate boxplot------------------------------------------------------------------------------------------------------\n",
    "        print('#################')\n",
    "        print(feature_name)\n",
    "        mygraph = data.boxplot(column=outcome_name,by=feature_name)\n",
    "        plt.ylabel(outcome_name)\n",
    "        plt.title('')\n",
    "        new_feature_name = feature_name.replace(\" \",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"*\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        new_feature_name = new_feature_name.replace(\"/\",\"\") #Deal with the dataset specific characters causing problems in this dataset.\n",
    "        plt.savefig('Exploratory_Plots/BoxPlot_'+str(new_feature_name)+'_'+outcome_name+'.pdf')\n",
    "        \n",
    "        #Univariate association test (Mann-Whitney Test - Non-parametric)\n",
    "        c, p = scs.mannwhitneyu(x=data[outcome_name].loc[data[feature_name] == '0'],y=data[outcome_name].loc[data[feature_name] == '1'])\n",
    "        print(\"Mann-Whitney P-value = \"+str(p))\n",
    "        print(\"Mann-Whitney U-Statistic = \"+str(c))\n",
    "\n",
    "        #Identify and save features with 'significant' univariate association\n",
    "        if p <= 0.05:\n",
    "            p_val_dict[feature_name] = p\n",
    "            \n",
    "    else:\n",
    "        print(\"Variable type combination not found! Check that all variables have been properly cast as either 'object' or 'float64'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates or ensures existance of folder to store exporatory plots. \n",
    "if not os.path.exists('Exploratory_Plots'):\n",
    "    os.makedirs('Exploratory_Plots')\n",
    "\n",
    "#Run appropriate univariate association analysis and generate an appropriate descriptive plot for each independent variable in the dataset. \n",
    "p_val_dict = {}\n",
    "\n",
    "for each in td:\n",
    "    if each != outcome_name: # Generate a plot for all independent variables. \n",
    "        graph_test_selector(each, outcome_name, td, p_val_dict)\n",
    "print('###################################################')\n",
    "print('Significant Univariate Associations:')\n",
    "for each in p_val_dict:\n",
    "    print(str(each)+\": (p-val = \"+str(p_val_dict[each])+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "Our knowledge of the dataset and it's target domain should be applied to perform a manual quality control check of the data.  Specifically do we observe any [outliers](https://en.wikipedia.org/wiki/Outlier) in any of the variables of the dataset? For instance we might check the 'Age at Diagnosis' variable and confirm that we don't see any ages outside of what would be reasonable for this target study (e.g. it's highly unlikely to observe anyone over the age of 110). Obvious highly unlikely or impossible outliers should be removed (i.e. either treated as a missing value, or the entire instance removed). These are often typos during data entry. \n",
    "\n",
    "* We don't observe any obvious 'impossible' outliers that need to be removed, however some statistical outliers are observed in the boxplots and in the basic descriptive summary statistics above.  For the purposes of this analysis we will not remove any of these outliers, however this is a consideration for followup analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Data Partitioning\n",
    "In order to rigourously evaluate our downstream predictive modeling it is important to partition our entire datasets (at minimum) into a training as well as a testing dataset. Even better if you have a sufficient number of instances in your dataset, it may be split into [training, validation, and testing sets](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7). Testing data will not be applied until modeling is complete in order to evaluate our model on data it has not yet seen.  This is critical to properly evaluating predictive success. \n",
    "\n",
    "A more rigorous approach is to perform [k-fold cross validation](https://machinelearningmastery.com/k-fold-cross-validation/), where analyses are performed on resampled partitions of the data, such that all instances serve as part of the testing set in at least one of the partitions.  This stragegy helps to avoid the potential bias introduced by evaluating a model on one random training/testing partition. \n",
    "\n",
    "For the purposes of notebook simplicity we will employ a single basic training/testing partition of the dataset. \n",
    "\n",
    "* There are many ways to go about partitioning and different implementations of this in Python. Some built in cross validation methods exist in scikit-learn but they do not easily allow you to save your respective training and testing datasets as files for external replication. \n",
    "* Often it may be easier to perform certain feature processing steps (e.g. feature transformation, feature engineering, and feature construction) prior to completing data partitioning. For the purposes of this notebook it is convenient to introduce and perform data partitioning here under 'Preprocessing'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the purposes of downstream ML recast all features previously assigned to be 'object' to 'int'\n",
    "td[header[0:23]] = td[header[0:23]].astype(dtype='int')\n",
    "td[['Class Attribute']] = td[['Class Attribute']].astype(dtype='int')\n",
    "td.dtypes\n",
    "#Partition data into a training and testing set using convenient scikit-learn command.\n",
    "train, test = train_test_split(td, test_size=0.2, stratify=td[outcome_name],random_state=randSeed) # 20% test set size, stratify option used to ensure class ratio is maintained in the partitions, random seed specified for reproducibility\n",
    "train.shape # Confirm dimensions of training set\n",
    "test.shape #Confirm dimensions of testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We confirm below that both the training and test sets have preserved the original class balance to help avoid this additional bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts of each class in training data\")\n",
    "train[outcome_name].value_counts()\n",
    "train[outcome_name].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Counts (Checking for Imbalance)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Counts of each class in testing data\")\n",
    "test[outcome_name].value_counts()\n",
    "test[outcome_name].value_counts().plot(kind='bar')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Counts (Checking for Imbalance)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# FEATURE PROCESSING\n",
    "Feature processing is all about improving the representation of data so that downstream machine learning can achieve higher performance. Feature processing can include any of the following: \n",
    "\n",
    "* [Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering) - New features are custom-built from existing features using domain knowledge. The goal is to provide downstream algorithms features that are more accessible for a machine to train on. (E.g. taking start and stop dates, that an algorithm would struggle to make sense of, and engineer a feature that captures the time duration between these dates. Feature engineering is typically completed manually using domain expertise. \n",
    "* [Feature Transformation](https://www.quora.com/What-is-a-feature-transformation-in-machine-learning) - Application of 'algorithms' to create new features from existing ones. \n",
    "    * [Scaling](https://en.wikipedia.org/wiki/Scaling_(geometry) - Mathematically [normalizing](https://en.wikipedia.org/wiki/Normalization_(statistics) features to fall within a given range or [transformed](https://en.wikipedia.org/wiki/Data_transformation_(statistics) by a given function (e.g. log transformation).  Scaling can be an important preprocessing step for interpreting the output of statistical modeling analysis (e.g. linear regression). Typically scaling is less important for machine learning (there are exceptions). \n",
    "    * [Binning](https://en.wikipedia.org/wiki/Data_binning) - converting continuous/real valued features to discrete or categorical features.\n",
    "    * [Dimensionality Reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction) - Also known as feature extraction or feature projection. Applies an algorithm designed to reduce the number of random variables under consideration by obtaining a set of principal variables, typically via some projection strategy (e.g. [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)).\n",
    "    * [Feature Construction](https://www.semanticscholar.org/paper/Feature-Construction-Methods-%3A-A-Survey-Sondhi/1faf80be961715c763bfab82d577e2a86ae65a9a) - Can be viewed as a more 'automated' approach to feature engineering, where a computational methods combines 2 or more existing features, or even the output of some other machine learning model to 'construct' a new feature.\n",
    "* [Feature Selection](https://en.wikipedia.org/wiki/Feature_selection) - The process of identifying a subset of 'relevant' and sometimes 'non-redundant' features to pass on to modeling.  Ideally feature selection will preserve all relevant features and eliminate all irrelevant or completely redundant features (however this is non-trivial). Three families of feature selection algorithms are typically described: filter-based methods, wrapper methods, and embedded methods. Notably, filter-based feature selection algorithms also serve as feature-weighting algorithms that can be employed as part of the exploratory analysis to assess which features are most likely to be informative. \n",
    "    * When applying filter-based feature selection it is generally up to the data scientist to decide how many variables to select, and how many to remove.  In datasets with very large numbers of features, it is usually a practical necessity to reduce the feature space as much as possible.  Different feature selection methods may offer some guidelines for when there is no evidence that a given feature is relevant (and thus may be removed from consideration). However, one must be aware of the inherant limitations of a given feature selection method before deciding whether to remove a feature, based on the goals or scope of the desired analysis (e.g. just because a feature selection method that can detect univariate associations suggests that a feature is irrelvant may not mean that a method that can take complex associations into account may identify it as being relevant). \n",
    "\n",
    "These components of an analysis pipeline are typically very problem/domain dependant, but are also tied to the modeling methods that plan to be utilized downstream (i.e. machine learning or traditional statistical modeling). In this notebook we will just demonstrate feature selection, as our dataset is relatively small (only 49 features), and our features do not require any obvious feature engineering or tranformation. \n",
    "\n",
    "* In particular, here we will try applying two different approaches: (1) [Mutual information](https://en.wikipedia.org/wiki/Mutual_information) an information theorgy approach for identifying features with a univariate association with outcome, (2) [ReliefF](https://en.wikipedia.org/wiki/Relief_(feature_selection) a filter-based feature selection algorithm that is uniquely able to indentify features with univariate associations as well as complex multivariate interactions that are predictive of outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Feature Scoring/Selection - Mutual Information\n",
    "Mutual information (MI) is a measure of the amount of information that one random variable has about another variable. \n",
    "MI between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean a strong dependency/association. \n",
    "Mutual Information \"is not concerned\" with whether the univariate association is linear or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataframe into features and outcome (standard format for scikit-learn methods)\n",
    "x_train = train.drop(outcome_name, axis=1).values\n",
    "y_train = train[outcome_name].values\n",
    "\n",
    "#Later when evaluating models we will need y_test so we will create it now...\n",
    "y_test = test[outcome_name].values\n",
    "\n",
    "#Run mutual information algorithm\n",
    "mi_results = mutual_info_classif(x_train, y_train, random_state=randSeed)\n",
    "\n",
    "#Present results\n",
    "header = train.columns.tolist()\n",
    "features = header[0:len(header)-1]\n",
    "names_scores = {'Names':features, 'Scores':mi_results} \n",
    "ns = pd.DataFrame(names_scores)\n",
    "ns = ns.sort_values(by='Scores')\n",
    "ns #Report sorted feature scores\n",
    "\n",
    "#Visualize sorted feature scores\n",
    "ns['Scores'].plot(kind='barh',figsize=(6,12))\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Mutual Information Score')\n",
    "plt.yticks(np.arange(len(features)), ns['Names'])\n",
    "plt.title('Sorted Mutual Information Scores')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection (Mutual Information)\n",
    "Any feature with a mutual information score at or close to zero is unlikely to have a univariate association with outcome. Therefore these features might be considered for removal.  Here we simply select all features with a mutual information score higher than zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_MI = []\n",
    "i=0\n",
    "for each in ns['Scores']:\n",
    "    if float(each) > 0.0:\n",
    "        #print(each)\n",
    "        selected_MI.append(ns['Names'][i])\n",
    "    i+=1 \n",
    "len(selected_MI)\n",
    "print(selected_MI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Feature Scoring/Selection - ReliefF\n",
    "ReliefF is a filter-based feature weighting and selection algorithm that utilizes a nearest neighbor component to scoring that allows it to indirectly estimate feature weights based not only univariate associations but also interacting multivariate associations. We employ a recently developed scikit-learn package called [scikit-rebate](https://github.com/EpistasisLab/scikit-rebate) which includes access to many Relief-based algorithms including the best known 'ReliefF'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float') #ReBATE cannot handle variables of type 'object' so we temporarily convert all features to type 'float'\n",
    "reliefF_results = ReliefF().fit(x_train, y_train) #ReliefF as a default 'k' hyperparameter that is set to 100 by default (i.e. 100 nearest neighbors)\n",
    "\n",
    "#Present results\n",
    "header = train.columns.tolist()\n",
    "features = header[0:len(header)-1]\n",
    "names_scores = {'Names':features, 'Scores':reliefF_results.feature_importances_} \n",
    "ns = pd.DataFrame(names_scores)\n",
    "ns = ns.sort_values(by='Scores')\n",
    "ns #Report sorted feature scores\n",
    "\n",
    "#Visualize sorted feature scores\n",
    "ns['Scores'].plot(kind='barh',figsize=(6,12))\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('ReliefF Score')\n",
    "plt.yticks(np.arange(len(features)), ns['Names'])\n",
    "plt.title('Sorted ReliefF Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection (ReliefF)\n",
    "Any ReliefF score at or below zero is unlikely to have an association with outcome. Therefore these features might be considered for removal.  Here we simply select all features with a ReliefF score higher than zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_ReliefF = []\n",
    "i=0\n",
    "for each in ns['Scores']:\n",
    "    if float(each) > 0.0:\n",
    "        #print(each)\n",
    "        selected_ReliefF.append(ns['Names'][i])\n",
    "    i+=1 \n",
    "len(selected_ReliefF)\n",
    "print(selected_ReliefF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our selection criteria for features in this notebook will be defined by the union of features identified as potentially relevant by either of our feature selection methods. This unique union of terms is determined and the respective training and testing data subsets are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_feature_names = list(set(selected_MI).union(selected_ReliefF))\n",
    "len(selected_feature_names)\n",
    "selected_feature_names\n",
    "selected_feature_names.sort()\n",
    "selected_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notably all of the features selected by Mutual Information were also selected by ReliefF, where MI identified a subset of the features identified by ReliefF. \n",
    "\n",
    "* It is often good practice to run ML modeling on both the orignial set of all features as well as the subset of selected features to confirm that performance was not negatively impacted by the feature selection choices. We will skip this for simplicity here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new training dataframes (filtered by feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter training data (these now only include features, not outcome)\n",
    "x_train_fs = train[selected_feature_names]\n",
    "x_train_fs.shape\n",
    "#Filter testing data (these now only include features, not outcome)\n",
    "x_test_fs = test[selected_feature_names]\n",
    "x_test_fs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING\n",
    "This is the stage that everyone thinks of when it comes to machine learning and data mining. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method Selection\n",
    "As a general rule of thumb it is typically best practice to run a number of machine learning algorithms in modeling (i.e. at least 2-3), and ideally those methods should have differen strengths and weaknesses.  This is due to the [no-free-lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) that essentially suggests that no one method can perform ideally in all circumstances.  Therefore, when we are faced with the analysis of some new dataset there is no way to know the optimal strategy to apply ahead of time. Sometimes simple analysis methods can work optimally (as well as run quickly with interpretable results), however other times much more complicated methods are required that may have different trade-offs. \n",
    "\n",
    "In this analysis/notebook we have decided to focus on two machine learning modeling approaches: (1) [decision trees](https://en.wikipedia.org/wiki/Decision_tree), and (2) [random forests](https://en.wikipedia.org/wiki/Random_forest). We have selected decision trees because the are an easier to understand and an intuitive ML modeling approach.  Further, the models produced by these systems are easy to interpret and apply. However decision trees are often suseptible to bias, and even minor instance sampling differences in the training set can alter the trained model, impacting model generalizability and reproducibility. As a secondary method, we focus on random forests, which are essentially a methodological extension of the decision tree concept. Random forests are one of the best known and most widely applied ML algorithms.  They introduce the concept of an ensemble, training a large group of trees to collectively constitute a predition model.  This allows these methods to be more robust to bias, obtain better generalization capability, and are still easily and quickly trained. However, random forests are inherently more difficult to interpret than a decision tree, and (like decision trees) struggle to model complex interactions in data particularly when the number of features gets to be large. \n",
    "\n",
    "* Below we start off by training both ML modeling algorithms on our target training data and report the standard training and testing accuracy of each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Modeling\n",
    "Like with most any ML algorithm you may have heard of, it isn't really a single specific method but a family of method variants.  The scikit-learn package implements a decision tree algorithm that is an "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree model. (no hyperparameters set other than random seed, i.e. default hyperparameters applied)\n",
    "dt = tree.DecisionTreeClassifier(random_state=randSeed)\n",
    "dt = dt.fit(x_train_fs, y_train)\n",
    "\n",
    "# Determine model's predictions on training data\n",
    "train_pred = dt.predict(x_train_fs)\n",
    "print(\"Training Accuracy:\",metrics.accuracy_score(y_train, train_pred))\n",
    "\n",
    "# Determine model's predictions on testing data\n",
    "test_pred = dt.predict(x_test_fs)\n",
    "print(\"Testing Accuracy:\",metrics.accuracy_score(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using a basic accuracy metric (i.e. (TP+TN)/(TP+TN+FP+FN)) our decision tree yields a training accuracy and a testing accuracy.  Keep in mind that our dataset is imbalanced so this evaluation metric is not optimal and our interpretation may be biased. We will revist this below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest model. (no hyperparameters set other than random seed, i.e. default hyperparameters applied)\n",
    "rf = RandomForestClassifier(random_state=randSeed)\n",
    "rf = rf.fit(x_train_fs, y_train)\n",
    "\n",
    "# Determine model's predictions on training data\n",
    "train_pred = rf.predict(x_train_fs)\n",
    "print(\"Training Accuracy:\",metrics.accuracy_score(y_train, train_pred))\n",
    "\n",
    "# Determine model's predictions on testing data\n",
    "test_pred = rf.predict(x_test_fs)\n",
    "print(\"Testing Accuracy:\",metrics.accuracy_score(y_test, test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Sweep\n",
    "Most any ML method has 'hyper' or 'run' parameters that the user can tune/adjust, modifying how the algorithm functions. The process of tweaking and optimizing these parameters to improve ML performance is known as [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization). A hyperparameter sweep mostly commonly includes idenifying which algorithm parameters are known to, or are likely to, impact performance on a given dataset, and then varying the setting of these one or more paramters in separate runs of the algorithm.  This step is not only important for squeezing the maximum performance out of ML methods, but also in fairly compairing the performance of different ML methods on given datasets.  \n",
    "\n",
    "Out of necessity, almost all ML algorithms have default hyperparameters specified (related to [default arguments](https://en.wikipedia.org/wiki/Default_argument). First and foremost, default parameters are specified so that the algorithm is able to run even if the user forgets to specify these hyperparamters. They are often set with the intention of testing out the method on a 'toy' or demonstration dataset included with the software, or they are set as simple, accessible placeholders to ensure that the first time a user runs the software it runs smoothly and quickly. However default parameters are certainly not guarenteed (if even likely) to lead to optimal ML performance on a given dataset. \n",
    "\n",
    "The most common and basic hyperparameter sweep strategies include a [grid search](https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998) or a [random search](https://en.wikipedia.org/wiki/Random_search), but other more sophisticated searches are also employed. \n",
    "\n",
    "Like with many other things in ML, there is no absolute right way to perform a hyperparameter sweep. One could easily waste a large amount of computational time on an exhaustive serach of hyperparameter settings and combinations without improving performance much, or at all. However some degree of hyperparameter searching outside of using the default hyperparameter settings is considered essential to ML best practices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - Random Sweep of Major Hyperparameters\n",
    "Here we consider some of the hyperparameters with the most potential to impact decision tree performance. \n",
    "\n",
    "* max_depth = The maximum depth of the tree. (default = None)\n",
    "* min_samples_split = The minimum number of samples required to split an internal node: (default = 2)\n",
    "* min_samples_leaf  = The minimum number of samples required to be at a leaf node. (default = 1)\n",
    "* criterion = The function to measure the quality of a split. (Default = 'gini')\n",
    "\n",
    "We employ a simple randomized hyperparameter search (with built in, internal cross validation - based on our training sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare a range/set of hyperparameter values for each\n",
    "param_grid = {\"max_depth\": [3, None], \"min_samples_split\": randint(2, 10), \"min_samples_leaf\": randint(1, 10), \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "model = tree.DecisionTreeClassifier(random_state=randSeed)\n",
    "#Specifics of the random sweep - up to 100 randomly selected hyperparameter combinations\n",
    "hp_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, random_state=randSeed)\n",
    "hp_search.fit(x_train_fs, y_train)\n",
    "\n",
    "# summarize the results of the random parameter search\n",
    "print(\"**************************\")\n",
    "print(hp_search.best_score_)\n",
    "print(\"************************************************************************************\")\n",
    "print(\"The model with the best accuracy was built with the following hyperparameter values.\")\n",
    "print(hp_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We retrain a decision tree on our training data using these 'optimal' parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new decision tree model. (hyperparameters set to those that yielded the best accuracy in the random sweep)\n",
    "dt = tree.DecisionTreeClassifier(max_depth=hp_search.best_params_['max_depth'], min_samples_leaf=hp_search.best_params_['min_samples_leaf'], min_samples_split=hp_search.best_params_['min_samples_split'], criterion=hp_search.best_params_['criterion'],random_state=randSeed)\n",
    "dt = dt.fit(x_train_fs, y_train)\n",
    "\n",
    "# Determine model's predictions on training data\n",
    "dt_train_pred = dt.predict(x_train_fs)\n",
    "print(\"Training Accuracy:\",metrics.accuracy_score(y_train, dt_train_pred))\n",
    "\n",
    "# Determine model's predictions on testing data\n",
    "dt_test_pred = dt.predict(x_test_fs)\n",
    "print(\"Testing Accuracy:\",metrics.accuracy_score(y_test, dt_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest - Random Sweep of Major Hyperparameters\n",
    "Here we consider some of the hyperparameters with the most potential to impact random forest performance. \n",
    "\n",
    "* n_estimators = The number of trees in the forest. (default = 10)\n",
    "* max_depth = The maximum depth of the tree. (default = None)\n",
    "* min_samples_split = The minimum number of samples required to split an internal node: (default = 2)\n",
    "* min_samples_leaf  = The minimum number of samples required to be at a leaf node. (default = 1)\n",
    "* criterion = The function to measure the quality of a split. (Default = 'gini')\n",
    "\n",
    "We employ a simple randomized hyperparameter search (with built in, internal cross validation - based on our training sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare a range/set of hyperparameter values for each\n",
    "param_grid = {\"n_estimators\": randint(2, 1000),\"max_depth\": [3, None], \"min_samples_split\": randint(2, 10), \"min_samples_leaf\": randint(1, 10), \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "model = RandomForestClassifier(random_state=randSeed)\n",
    "#Specifics of the random sweep - up to 100 randomly selected hyperparameter combinations\n",
    "hp_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)\n",
    "hp_search.fit(x_train_fs, y_train)\n",
    "\n",
    "# summarize the results of the random parameter search\n",
    "print(\"**************************\")\n",
    "print(hp_search.best_score_)\n",
    "print(\"************************************************************************************\")\n",
    "print(\"The model with the best accuracy was built with the following hyperparameter values.\")\n",
    "print(hp_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We retrain a random forest on our training data using these 'optimal' parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new random forest model. (hyperparameters set to those that yielded the best accuracy in the random sweep)\n",
    "rf = RandomForestClassifier(n_estimators = hp_search.best_params_['n_estimators'], max_depth=hp_search.best_params_['max_depth'], min_samples_leaf=hp_search.best_params_['min_samples_leaf'], min_samples_split=hp_search.best_params_['min_samples_split'], criterion=hp_search.best_params_['criterion'],random_state=randSeed)\n",
    "rf = rf.fit(x_train_fs, y_train)\n",
    "\n",
    "# Determine model's predictions on training data\n",
    "rf_train_pred = rf.predict(x_train_fs)\n",
    "print(\"Training Accuracy:\",metrics.accuracy_score(y_train, rf_train_pred))\n",
    "\n",
    "# Determine model's predictions on testing data\n",
    "rf_test_pred = rf.predict(x_test_fs)\n",
    "print(\"Testing Accuracy:\",metrics.accuracy_score(y_test, rf_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that we have trained models, how do we best evaluate and compare the performance of these models?  Proper evaluation is critical to making good conclusions. Many metrics and measures of model 'goodness' exist. Part of what differentiates these evaluation methods is understanding the assumptions being made by a given metric, or what the metric prioritizes as important for model goodness. A nice review of key evaluation metrics can be found [here](http://www.davidsbatista.net/blog/2018/08/19/NLP_Metrics/). \n",
    "\n",
    "Below we will apply some different evaluation methods to the decision tree model that was trained using the 'optimal' parameters settings indentified in the random sweeep (we will leave out the random forest evaluation here for brevity). All evaluations will focus on predictive performance on the testing data. \n",
    "\n",
    "The metrics are typically based on calculations using the different possible classification predictions that can be made: True Positives (TP), True Negative (TN), False Positive (FP), and False Negative (FN).  See [here](https://towardsdatascience.com/the-mystery-of-true-positive-true-negative-false-positive-and-false-negative-fd73c78c905a) to review TP, TN, FP, FN.\n",
    "\n",
    "Let's start by calculating each of these for our test data using our 'optimized' decision tree model. Recall that our dataset is imbalanced and there are more 'negatives' than 'positives'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate confusion matrix (true outcomes, predicted outcomes)\n",
    "TN, FP, FN, TP = confusion_matrix(y_test, dt_test_pred).ravel()\n",
    "\n",
    "print(\"TP = \"+str(TP))\n",
    "print(\"FP = \"+str(FP))\n",
    "print(\"FN = \"+str(FN))\n",
    "print(\"TN = \"+str(TN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values (TP,FP,FN,TN) are components used to calculate a number of different evaluation metrics for classification tasks:\n",
    "\n",
    "* Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "* Precision (a.k.a. Sensitivity)= TP/(TP+FP)\n",
    "* Recall (a.k.a. True Positive Rate) = TP/(TP+FN)\n",
    "* Specificity = TN/(TN+FP)\n",
    "* False Positive Rate = FP/(FP+TN)\n",
    "\n",
    "* F1 Score = 2*(Precision * Recall)/(Precision + Recall)\n",
    "* Balanced Accuracy = (Sensitivity + Specificity)/2\n",
    "\n",
    "We will calculate these for our 'optimized' decision tree model.  Be aware that a different set of evaluation metrics would be used for continuous-valued outcomes (i.e. regression tasks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine model's predictions on testing data\n",
    "print(\"All metrics report performance on testing data!\")\n",
    "print(\"***********************************************\")\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, dt_test_pred))\n",
    "print(\"***********************************************\")\n",
    "\n",
    "#Generate a classification report\n",
    "report = classification_report(y_test, dt_test_pred)\n",
    "print (report)\n",
    "print(\"***********************************************\")\n",
    "specificity = TN/(TN+FP)\n",
    "sensitivity = TP/(TP+FP)\n",
    "print(\"Specificity: \"+str(specificity))\n",
    "print(\"***********************************************\")\n",
    "print(\"False Positive Rate: \"+str(FP/(TN+FP))+ \"   Note: Lower is better.\")\n",
    "print(\"***********************************************\")\n",
    "print(\"Balanced Accuracy: \"+str((sensitivity+specificity)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For imbalanced datasets, balanced accuracy is less biased in comparing performance as it weights the accurate prediction of both classes equally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve and AUC metric\n",
    "The ROC curve is a metric used for the evaluation of binary classification models, where we evaluate how the model would perform if different cuttoff thresholds (of predicted class probability) between the two classes were considered. This is useful when we don't necessarily know which class is more important to predict accurately. For example, if we wanted to evaluate how well a test for a highly infectious disease worked, we would care much more about identifying positive individuals correctly than negative individuals, even at the expense of additional false positives.\n",
    "\n",
    "To calculate an ROC curve first we need to get the class prediction probabilites from the model, rather than just the predicted classes, since these predicted classes were decided based on a predetermined classification threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Determine probabilities of class predictions for each test instance (this will be used much later in calculating an ROC curve)\n",
    "probas_ = dt.fit(x_train_fs, y_train).predict_proba(x_test_fs)\n",
    "\n",
    "# Compute ROC curve and area the curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#Plot the ROC Curve and include AUC in figure. \n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall Plot - An alternative to ROC when there is class imbalance\n",
    "While we have successuflly generated an ROC curve above, when there is a moderate to large class imbalance it is considered better practice to evaluate models with a precision/recall plot (PRC) ([see here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and area the curve\n",
    "precision, recall, thresholds = metrics.precision_recall_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(recall,precision)\n",
    "\n",
    "# calculate average precision score\n",
    "ap = metrics.average_precision_score(y_test, probas_[:, 1])\n",
    "\n",
    "print(\"AUC of PRC = \"+ str(roc_auc))\n",
    "print(\"Average Precision = \"+ str(ap))\n",
    "\n",
    "# plot no skill - based on the imbalance ratio. Our ratio is 1:1.61 which translates to a no skill line of about .383\n",
    "pyplot.plot([0, 1], [0.383, 0.383], linestyle='--')\n",
    "# plot the precision-recall curve for the model\n",
    "pyplot.plot(recall, precision, marker='.')\n",
    "pyplot.xlabel(\"Recall\")\n",
    "pyplot.ylabel(\"Precision\")\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSTPROCESSING\n",
    "Here we discuss some of the last steps in the machine learning analysis pipeline geared towards understanding our model, drawing conclusions, deriving signifiance statistics on our results, or preparing our predictive model for deployment. Here we highlight some of the basics of interpreting our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "In some problem domains it may be sufficient to identify an accurate predictive model.  However in many other domains in order for anyone to have faith in the ability of the model to perform when applied to as yet unseen data, it's important to understand our model, including:\n",
    "* What features are driving accurate predictions? (i.e. which features a truly relevant?)\n",
    "* What is the nature of any multivariate associations (i.e. are there additive univariate effects, specific feature interactions, heterogeneous associations, or some combined complex relationship between relevant features and outcome?\n",
    "* Can we understand the decision making process in making a prediction on a given instance? Does the model make sense based on our current understanding of the problem domain?\n",
    "* Does the model lead us to any new understandings or insights about he problem domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Decision Tree Model\n",
    "Generate a visualization of the decision tree showing all nodes, splits, and leaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(dt, out_file=None,feature_names=selected_feature_names, class_names=['0','1']) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"decisionTree\") \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Decision Tree Model - (the round about way)\n",
    "To generate a visualization of the decision tree but are struggling with getting the graphviz python package to work, go through the following steps:\n",
    "1) Run the next line of code below and a '.dot' file will be produced (you can open this with any text editor)\n",
    "\n",
    "2) Copy the text in this file\n",
    "\n",
    "3) Go to the following [weblink](http://webgraphviz.com/)\n",
    "\n",
    "4) Paste the text in the Graphviz window.\n",
    "\n",
    "5) Click 'generate graph'\n",
    "\n",
    "6) Take screenshot of figure, paste in Powerpoint, crop, and save as picture. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually generate tree visualization\n",
    "dotfile = open(\"dt.dot\", 'w')\n",
    "tree.export_graphviz(dt, out_file=dotfile, feature_names=selected_feature_names)\n",
    "dotfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive Rules from Decision Tree\n",
    "We can also seek to extract human readible rule expressions from any branch paths from the root node (top) of the tree down to any given leaf node (bottom). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_code(tree, feature_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    #print(\"def tree({}):\".format(\", \".join(feature_names)))\n",
    "\n",
    "    def recurse(node, depth):\n",
    "        indent = \"  \" * depth\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            print(\"{}if {} <= {}:\".format(indent, name, threshold))\n",
    "            recurse(tree_.children_left[node], depth + 1)\n",
    "            print(\"{}else:  # if {} > {}\".format(indent, name, threshold))\n",
    "            recurse(tree_.children_right[node], depth + 1)\n",
    "        else:\n",
    "            print(\"{}return {}\".format(indent, tree_.value[node]))\n",
    "\n",
    "    recurse(0, 1)\n",
    "\n",
    "tree_to_code(dt,selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Scores - Random Forest\n",
    "Here we return to our random forest model to examine a commonly used output to help with model interpretation, i.e. feature importance scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "names_importances = {'Names':selected_feature_names, 'Scores':importances} \n",
    "ni = pd.DataFrame(names_importances)\n",
    "ni = ni.sort_values(by='Scores')\n",
    "ni.shape\n",
    "ni #Report sorted feature scores\n",
    "\n",
    "#Visualize sorted feature scores\n",
    "ni['Scores'].plot(kind='barh',figsize=(6,12))\n",
    "plt.ylabel('Features')\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.yticks(np.arange(len(features)), ni['Names'])\n",
    "plt.title('Sorted Feature Importance Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance Analysis\n",
    "When conducting a machine learning analysis on a stand-alone dataset, we may ask the question whether the model evaluation metrics we observed or feature selection weights, or the feature importance scores, etc are signifianctly higher than we might expect by chance within the given datasets.  This is where strategies like [permatuation testing](https://en.wikipedia.org/wiki/Resampling_(statistics)) can play a role in evaluating statistical significance of ML results. We won't complete permuation testing or additional significance anlaysis here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication\n",
    "The gold standard in any machine learnin analysis is to be able to replicate similar findings in a secondary dataset. For the dataset targeted in this notebook, no replication datasets are readily available so we will not conduct a replication analysis here. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
